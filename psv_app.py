
import os
import uuid
from datetime import timedelta
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import matplotlib as mpl
from matplotlib import font_manager


# ================== Config ==================
SET_P = 1.32
DATA_FILE = "psv_data.csv"
ALERT_FILE = "psv_alerts.csv"
AUDIT_FILE = "psv_audit_logs.csv"

TABLE_DATA = "psv_data"
TABLE_ALERT = "psv_alerts"
TABLE_AUDIT = "psv_audit_logs"

STATUS_FLOW = ["å¾…ç¡®è®¤", "å·²æ´¾å·¥", "å¤„ç†ä¸­", "å·²éªŒè¯", "å·²å…³é—­"]
STATIONS = ["åç›˜LNGåŠ æ°”ç«™", "ç½—æ‰€LNGåŠ æ°”ç«™"]
DEFAULT_STATION = "åç›˜LNGåŠ æ°”ç«™"

AI_FEATURES = ["p_now", "p_max", "ratio", "slope_3d", "level", "temp", "Activity"]
AI_FEATURE_LABELS = {
    "p_now": "å½“å‰å‹åŠ›",
    "p_max": "æœ€é«˜å‹åŠ›",
    "ratio": "æ¥è¿‘æ•´å®šæ¯”",
    "slope_3d": "3æ—¥å‹åŠ›æ–œç‡",
    "level": "æ¶²ä½",
    "temp": "æ¸©åº¦",
    "Activity": "åŠ¨ä½œ/å¾®æ”¾æ•£",
}


# ================== Supabase init ==================
try:
    from supabase import create_client

    SUPABASE_OK = True
except Exception:
    SUPABASE_OK = False


def _secret_get(key: str, default=""):
    try:
        return st.secrets.get(key, default)
    except Exception:
        return default


SUPABASE_URL = _secret_get("SUPABASE_URL", "") or os.getenv("SUPABASE_URL", "")
SUPABASE_KEY = _secret_get("SUPABASE_KEY", "") or os.getenv("SUPABASE_KEY", "")

USE_SUPABASE = True
supabase = None
if USE_SUPABASE and SUPABASE_OK and SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
elif USE_SUPABASE:
    st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ° Supabase é…ç½®ï¼ˆSUPABASE_URL / SUPABASE_KEYï¼‰ï¼Œå°†å›é€€ä¸ºæœ¬åœ°CSVå­˜å‚¨ã€‚")
    USE_SUPABASE = False


# ================== ML ==================
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler

    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False


# ================== Font ==================
def _setup_cjk_font():
    preferred_font_names = [
        "Noto Sans CJK SC",
        "Noto Sans CJK JP",
        "Noto Sans CJK TC",
        "Microsoft YaHei",
        "SimHei",
        "PingFang SC",
        "WenQuanYi Micro Hei",
        "Source Han Sans SC",
    ]

    available = {f.name for f in font_manager.fontManager.ttflist}
    for name in preferred_font_names:
        if name in available:
            mpl.rcParams["font.family"] = "sans-serif"
            mpl.rcParams["font.sans-serif"] = [name, "DejaVu Sans"]
            mpl.rcParams["axes.unicode_minus"] = False
            return

    font_path = "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc"
    if os.path.exists(font_path):
        fp = font_manager.FontProperties(fname=font_path)
        mpl.rcParams["font.family"] = fp.get_name()
        mpl.rcParams["axes.unicode_minus"] = False


_setup_cjk_font()
st.set_page_config(page_title="Isolation Forest LNGå®‰å…¨é˜€AIé¢„è­¦ç³»ç»Ÿ", layout="wide")


# ================== Auth ==================
def load_accounts() -> dict:
    hp = _secret_get("PASSWORD_HUAPAN", "hp123456") or os.getenv("PASSWORD_HUAPAN", "hp123456")
    ls = _secret_get("PASSWORD_LUOSUO", "ls123456") or os.getenv("PASSWORD_LUOSUO", "ls123456")
    leader = _secret_get("PASSWORD_LEADER", "leader123456") or os.getenv("PASSWORD_LEADER", "leader123456")

    return {
        "åç›˜ç«™": {"password": hp, "role": "station", "station_scope": "åç›˜LNGåŠ æ°”ç«™"},
        "ç½—æ‰€ç«™": {"password": ls, "role": "station", "station_scope": "ç½—æ‰€LNGåŠ æ°”ç«™"},
        "é¢†å¯¼": {"password": leader, "role": "leader", "station_scope": "ALL"},
    }


ACCOUNTS = load_accounts()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False
if "user_name" not in st.session_state:
    st.session_state.user_name = ""
if "role" not in st.session_state:
    st.session_state.role = ""
if "station_scope" not in st.session_state:
    st.session_state.station_scope = ""

st.sidebar.title("ğŸ” è®¿é—®æ§åˆ¶")

if not st.session_state.authenticated:
    login_name = st.sidebar.selectbox("è´¦å·", list(ACCOUNTS.keys()), index=0)
    login_pwd = st.sidebar.text_input("å¯†ç ", type="password")
    if st.sidebar.button("ç™»å½•", use_container_width=True):
        if login_pwd == ACCOUNTS[login_name]["password"]:
            st.session_state.authenticated = True
            st.session_state.user_name = login_name
            st.session_state.role = ACCOUNTS[login_name]["role"]
            st.session_state.station_scope = ACCOUNTS[login_name]["station_scope"]
            st.rerun()
        else:
            st.sidebar.error("å¯†ç é”™è¯¯")
    st.warning("è¯·è¾“å…¥è´¦å·å’Œå¯†ç åè¿›å…¥ç³»ç»Ÿã€‚")
    st.stop()
else:
    st.sidebar.success(
        f"å·²ç™»å½•ï¼š{st.session_state.user_name} | è§’è‰²ï¼š{st.session_state.role} | èŒƒå›´ï¼š{st.session_state.station_scope}"
    )
    if st.sidebar.button("é€€å‡ºç™»å½•", use_container_width=True):
        for k in ["authenticated", "user_name", "role", "station_scope"]:
            st.session_state.pop(k, None)
        st.rerun()


ROLE = st.session_state.role
STATION_SCOPE = st.session_state.station_scope
IS_LEADER = ROLE == "leader"


# ================== Data helpers ==================
BASE_DATA_COLS = [
    "date",
    "station",
    "valve_type",
    "p_now",
    "p_max",
    "level",
    "temp",
    "psv_act",
    "psv_weeping",
    "operator_role",
    "operator_name",
    "updated_at",
]

BASE_ALERT_COLS = [
    "id",
    "date",
    "station",
    "valve_type",
    "risk_level",
    "trigger_source",
    "trigger_detail",
    "status",
    "owner",
    "action_taken",
    "verification_result",
    "created_at",
    "updated_at",
    "closed_at",
]

BASE_AUDIT_COLS = ["id", "entity_type", "entity_id", "action", "operator", "payload", "created_at"]


def _ensure_local_files():
    if not os.path.exists(DATA_FILE):
        pd.DataFrame(columns=BASE_DATA_COLS).to_csv(DATA_FILE, index=False, encoding="utf-8-sig")
    if not os.path.exists(ALERT_FILE):
        pd.DataFrame(columns=BASE_ALERT_COLS).to_csv(ALERT_FILE, index=False, encoding="utf-8-sig")
    if not os.path.exists(AUDIT_FILE):
        pd.DataFrame(columns=BASE_AUDIT_COLS).to_csv(AUDIT_FILE, index=False, encoding="utf-8-sig")


_ensure_local_files()


def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or len(df) == 0:
        return df

    rename_map = {
        "Valve_type": "valve_type",
        "P_now": "p_now",
        "P_max": "p_max",
        "Level": "level",
        "Temp": "temp",
        "PSV_act": "psv_act",
        "PSV_weeping": "psv_weeping",
    }
    df = df.rename(columns=rename_map)

    for c in BASE_DATA_COLS:
        if c not in df.columns:
            if c == "station":
                df[c] = DEFAULT_STATION
            elif c in ["operator_role", "operator_name", "updated_at"]:
                df[c] = ""
            else:
                df[c] = np.nan

    return df[BASE_DATA_COLS]


def _normalize_df(df0: pd.DataFrame) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return pd.DataFrame(columns=BASE_DATA_COLS)

    df = _standardize_columns(df0.copy())
    df["station"] = df["station"].fillna(DEFAULT_STATION).replace("", DEFAULT_STATION)
    df["station"] = df["station"].where(df["station"].isin(STATIONS), DEFAULT_STATION)

    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date

    for col in ["p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df["p_now"] = df["p_now"].clip(lower=0, upper=2)
    df["p_max"] = df["p_max"].clip(lower=0, upper=2)
    df["level"] = df["level"].clip(lower=0, upper=100)
    df["temp"] = df["temp"].clip(lower=-50, upper=80)

    m = df["p_max"].notna() & df["p_now"].notna() & (df["p_max"] < df["p_now"])
    if m.any():
        df.loc[m, "p_max"] = df.loc[m, "p_now"]

    df = df.dropna(subset=["date", "station", "valve_type", "p_max"])
    df = (
        df.sort_values(["station", "valve_type", "date", "updated_at"])
        .drop_duplicates(subset=["date", "station", "valve_type"], keep="last")
        .reset_index(drop=True)
    )

    return df


def _scope_filter(df: pd.DataFrame, station_scope: str) -> pd.DataFrame:
    if len(df) == 0:
        return df
    if station_scope == "ALL":
        return df
    return df[df["station"] == station_scope].copy()


def load_data(station_scope: str, role: str) -> pd.DataFrame:
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table(TABLE_DATA).select("*").execute()
        raw = pd.DataFrame(resp.data or [])
    else:
        raw = pd.read_csv(DATA_FILE)

    df = _normalize_df(raw)
    return _scope_filter(df, station_scope)


def _write_local_data(df: pd.DataFrame):
    df.to_csv(DATA_FILE, index=False, encoding="utf-8-sig")


def save_record(record: dict, station_scope: str, role: str) -> None:
    if role == "leader":
        raise PermissionError("é¢†å¯¼è´¦å·ä¸ºåªè¯»ï¼Œä¸å…è®¸å†™å…¥æ•°æ®")

    if station_scope != "ALL" and record.get("station") != station_scope:
        raise PermissionError("åªèƒ½å†™å…¥æœ¬ç«™æ•°æ®")

    record = record.copy()
    record["updated_at"] = pd.Timestamp.now().isoformat()

    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_DATA).upsert(record, on_conflict="date,station,valve_type").execute()
        return

    df_all = _normalize_df(pd.read_csv(DATA_FILE))
    new_row = pd.DataFrame([record])
    merged = pd.concat([df_all, new_row], ignore_index=True)
    merged = _normalize_df(merged)
    _write_local_data(merged)

# ================== Alerts ==================
def _normalize_alert_df(df0: pd.DataFrame) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return pd.DataFrame(columns=BASE_ALERT_COLS)

    df = df0.copy()
    for c in BASE_ALERT_COLS:
        if c not in df.columns:
            df[c] = ""

    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
    df["station"] = df["station"].fillna(DEFAULT_STATION).replace("", DEFAULT_STATION)
    df["status"] = df["status"].replace("", "å¾…ç¡®è®¤")
    return df[BASE_ALERT_COLS]


def _load_alerts_all() -> pd.DataFrame:
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table(TABLE_ALERT).select("*").execute()
        raw = pd.DataFrame(resp.data or [])
    else:
        raw = pd.read_csv(ALERT_FILE)
    return _normalize_alert_df(raw)


def _save_alerts_local(df: pd.DataFrame):
    df.to_csv(ALERT_FILE, index=False, encoding="utf-8-sig")


def append_audit(entity_type: str, entity_id: str, action: str, operator: str, payload: str):
    log = {
        "id": str(uuid.uuid4()),
        "entity_type": entity_type,
        "entity_id": str(entity_id),
        "action": action,
        "operator": operator,
        "payload": payload,
        "created_at": pd.Timestamp.now().isoformat(),
    }
    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_AUDIT).insert(log).execute()
    else:
        df = pd.read_csv(AUDIT_FILE)
        df = pd.concat([df, pd.DataFrame([log])], ignore_index=True)
        df.to_csv(AUDIT_FILE, index=False, encoding="utf-8-sig")


def _find_alert(alerts: pd.DataFrame, date_value, station: str, valve_type: str):
    m = (
        (alerts["date"] == pd.to_datetime(date_value).date())
        & (alerts["station"] == station)
        & (alerts["valve_type"] == valve_type)
    )
    return alerts[m]


def create_or_update_alert(record: dict):
    now_iso = pd.Timestamp.now().isoformat()
    alerts = _load_alerts_all()

    found = _find_alert(alerts, record["date"], record["station"], record["valve_type"])
    if len(found) > 0:
        idx = found.index[0]
        keep_status = alerts.loc[idx, "status"] or "å¾…ç¡®è®¤"
        alerts.loc[idx, "risk_level"] = record.get("risk_level", alerts.loc[idx, "risk_level"])
        alerts.loc[idx, "trigger_source"] = record.get("trigger_source", alerts.loc[idx, "trigger_source"])
        alerts.loc[idx, "trigger_detail"] = str(record.get("trigger_detail", alerts.loc[idx, "trigger_detail"]))
        alerts.loc[idx, "updated_at"] = now_iso
        alerts.loc[idx, "status"] = keep_status
    else:
        new_alert = {
            "id": str(uuid.uuid4()),
            "date": str(record["date"]),
            "station": record["station"],
            "valve_type": record["valve_type"],
            "risk_level": record.get("risk_level", "ğŸ”´ é«˜é£é™©"),
            "trigger_source": record.get("trigger_source", "rule"),
            "trigger_detail": str(record.get("trigger_detail", "")),
            "status": "å¾…ç¡®è®¤",
            "owner": "",
            "action_taken": "",
            "verification_result": "",
            "created_at": now_iso,
            "updated_at": now_iso,
            "closed_at": "",
        }
        alerts = pd.concat([alerts, pd.DataFrame([new_alert])], ignore_index=True)

    if USE_SUPABASE and supabase is not None:
        rows = alerts.to_dict(orient="records")
        supabase.table(TABLE_ALERT).upsert(rows, on_conflict="id").execute()
    else:
        _save_alerts_local(alerts)


def update_alert_status(alert_id: str, new_status: str, operator: str, action_taken: str = "", verification_result: str = ""):
    alerts = _load_alerts_all()
    hit = alerts[alerts["id"].astype(str) == str(alert_id)]
    if len(hit) == 0:
        raise ValueError("æœªæ‰¾åˆ°å‘Šè­¦")

    idx = hit.index[0]
    cur = alerts.loc[idx, "status"]
    if cur not in STATUS_FLOW:
        cur = "å¾…ç¡®è®¤"
    if new_status not in STATUS_FLOW:
        raise ValueError("éæ³•çŠ¶æ€")

    cur_i = STATUS_FLOW.index(cur)
    new_i = STATUS_FLOW.index(new_status)
    if not (new_i == cur_i or new_i == cur_i + 1):
        raise ValueError("çŠ¶æ€ä»…å…è®¸ä¿æŒä¸å˜æˆ–æ¨è¿›ä¸€æ­¥")

    if new_status == "å·²å…³é—­":
        if not action_taken.strip() or not verification_result.strip():
            raise ValueError("å…³é—­å‘Šè­¦å‰å¿…é¡»å¡«å†™æ•´æ”¹æªæ–½å’Œå¤éªŒç»“æœ")
        alerts.loc[idx, "closed_at"] = pd.Timestamp.now().isoformat()

    if action_taken.strip():
        alerts.loc[idx, "action_taken"] = action_taken.strip()
    if verification_result.strip():
        alerts.loc[idx, "verification_result"] = verification_result.strip()

    alerts.loc[idx, "status"] = new_status
    alerts.loc[idx, "owner"] = operator
    alerts.loc[idx, "updated_at"] = pd.Timestamp.now().isoformat()

    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_ALERT).upsert(alerts.to_dict(orient="records"), on_conflict="id").execute()
    else:
        _save_alerts_local(alerts)

    append_audit(
        entity_type="alert",
        entity_id=str(alert_id),
        action=f"status:{cur}->{new_status}",
        operator=operator,
        payload=f"action_taken={action_taken}; verification_result={verification_result}",
    )


def list_alerts(station_scope: str, role: str) -> pd.DataFrame:
    alerts = _load_alerts_all()
    if station_scope != "ALL":
        alerts = alerts[alerts["station"] == station_scope].copy()
    if len(alerts) == 0:
        return alerts
    return alerts.sort_values(["status", "date"], ascending=[True, False]).reset_index(drop=True)


# ================== Scoring ==================
def _risk_from_hi(x: float) -> str:
    if x >= 85:
        return "ğŸŸ¢ å®‰å…¨"
    if x >= 70:
        return "ğŸŸ¡ é¢„è­¦"
    return "ğŸ”´ é«˜é£é™©"


def build_ai_reason_top_features(row: pd.Series, group_stats: Dict[str, pd.Series]) -> Tuple[str, str, str]:
    med = group_stats.get("median", pd.Series(dtype=float))
    std = group_stats.get("std", pd.Series(dtype=float))

    scores = {}
    for f in AI_FEATURES:
        v = pd.to_numeric(pd.Series([row.get(f, np.nan)]), errors="coerce").iloc[0]
        m = pd.to_numeric(pd.Series([med.get(f, np.nan)]), errors="coerce").iloc[0]
        s = pd.to_numeric(pd.Series([std.get(f, np.nan)]), errors="coerce").iloc[0]
        if pd.isna(v) or pd.isna(m):
            scores[f] = -np.inf
            continue
        if pd.isna(s) or float(s) == 0:
            s = 1.0
        scores[f] = abs((float(v) - float(m)) / float(s))

    top = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)[:3]
    labels = [f"{AI_FEATURE_LABELS.get(k, k)}åç¦»" if np.isfinite(v) else "-" for k, v in top]
    while len(labels) < 3:
        labels.append("-")
    return labels[0], labels[1], labels[2]


def compute_iforest_signals(df_grouped: pd.DataFrame, contamination: float, window_days: int = 60, min_samples: int = 30) -> pd.DataFrame:
    df = df_grouped.copy()
    df["ai_raw_score"] = np.nan
    df["ai_score_pct"] = np.nan
    df["ai_observe_flag"] = False
    df["ai_escalate_flag"] = False
    df["ai_reason_top1"] = "-"
    df["ai_reason_top2"] = "-"
    df["ai_reason_top3"] = "-"

    if len(df) == 0 or not SKLEARN_OK:
        df["AI_anomaly"] = False
        df["AI_score"] = np.nan
        return df

    for _, g in df.groupby(["station", "valve_type"], sort=False):
        g = g.sort_values("date")
        ordered_idx = list(g.index)
        stats_cache = {}

        for pos, idx in enumerate(ordered_idx):
            win_idx = ordered_idx[max(0, pos - (window_days - 1)) : pos + 1]
            xw = g.loc[win_idx, AI_FEATURES].copy()
            xw = xw.apply(pd.to_numeric, errors="coerce")
            med = xw.median(numeric_only=True)
            xw = xw.fillna(med).fillna(0.0)

            if len(xw) < min_samples:
                continue

            std = xw.std(numeric_only=True).replace(0, np.nan)

            scaler = StandardScaler()
            xs = scaler.fit_transform(xw.values)

            iso = IsolationForest(
                n_estimators=300,
                contamination=float(contamination),
                random_state=42,
            )
            iso.fit(xs)

            cur_x = xs[-1].reshape(1, -1)
            raw = float(-iso.score_samples(cur_x)[0])
            df.at[idx, "ai_raw_score"] = raw
            stats_cache[idx] = {"median": med, "std": std}

        valid_idx = [i for i in ordered_idx if pd.notna(df.at[i, "ai_raw_score"])]
        if len(valid_idx) == 0:
            continue

        raw_s = pd.to_numeric(df.loc[valid_idx, "ai_raw_score"], errors="coerce")
        pct = raw_s.rank(pct=True, method="average") * 100
        df.loc[valid_idx, "ai_score_pct"] = pct
        df.loc[valid_idx, "ai_observe_flag"] = df.loc[valid_idx, "ai_score_pct"] >= 95

        for pos, idx in enumerate(ordered_idx):
            if idx not in valid_idx:
                continue

            observe = bool(df.at[idx, "ai_observe_flag"])
            if not observe:
                continue

            cur_date = pd.to_datetime(df.at[idx, "date"], errors="coerce")
            rule_risky = str(df.at[idx, "Risk"]) in ["ğŸŸ¡ é¢„è­¦", "ğŸ”´ é«˜é£é™©"]

            prev_consecutive = False
            if pos > 0:
                prev_idx = ordered_idx[pos - 1]
                prev_observe = bool(df.at[prev_idx, "ai_observe_flag"])
                prev_date = pd.to_datetime(df.at[prev_idx, "date"], errors="coerce")
                if pd.notna(cur_date) and pd.notna(prev_date):
                    prev_consecutive = prev_observe and ((cur_date.date() - prev_date.date()).days == 1)

            escalate = prev_consecutive or rule_risky
            df.at[idx, "ai_escalate_flag"] = bool(escalate)

            t1, t2, t3 = build_ai_reason_top_features(df.loc[idx], stats_cache.get(idx, {}))
            df.at[idx, "ai_reason_top1"] = t1
            df.at[idx, "ai_reason_top2"] = t2
            df.at[idx, "ai_reason_top3"] = t3

    df["AI_anomaly"] = df["ai_observe_flag"].fillna(False).astype(bool)
    df["AI_score"] = df["ai_raw_score"]
    return df


def compute_scores(df0: pd.DataFrame, enable_ai: bool, contamination: float) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return df0

    df = _normalize_df(df0).copy()
    df = df.sort_values(["station", "valve_type", "date"]).reset_index(drop=True)

    df["ratio"] = df["p_max"] / SET_P
    df["slope_3d"] = (
        df.groupby(["station", "valve_type"])["p_max"]
        .apply(lambda s: s.diff().rolling(3).mean())
        .reset_index(level=[0, 1], drop=True)
    )
    df["slope"] = df["slope_3d"]

    hi = np.full(len(df), 100.0)
    hi -= np.where(df["ratio"] >= 1.00, 35, 0)
    hi -= np.where((df["ratio"] >= 0.98) & (df["ratio"] < 1.00), 20, 0)
    hi -= np.where((df["ratio"] >= 0.95) & (df["ratio"] < 0.98), 10, 0)

    hi -= np.where(df["slope_3d"] > 0.01, 10, 0)
    hi -= np.where(df["slope_3d"] > 0.02, 10, 0)

    hi -= df.get("psv_act", 0).fillna(0) * 30
    hi -= df.get("psv_weeping", 0).fillna(0) * 15

    hi -= np.where(
        (df.get("temp", 0).fillna(0) >= 33)
        & (df.get("level", 0).fillna(0) >= 80)
        & (df["ratio"] >= 0.95),
        10,
        0,
    )

    df["HI"] = np.clip(hi, 0, 100)
    df["Risk"] = df["HI"].apply(_risk_from_hi)
    df["Activity"] = df.get("psv_act", 0).fillna(0) + df.get("psv_weeping", 0).fillna(0)

    df["ai_raw_score"] = np.nan
    df["ai_score_pct"] = np.nan
    df["ai_observe_flag"] = False
    df["ai_escalate_flag"] = False
    df["ai_reason_top1"] = "-"
    df["ai_reason_top2"] = "-"
    df["ai_reason_top3"] = "-"
    df["AI_anomaly"] = False
    df["AI_score"] = np.nan

    if enable_ai and SKLEARN_OK:
        df = compute_iforest_signals(df, contamination=contamination, window_days=60, min_samples=30)
        obs = df["ai_observe_flag"].fillna(False).astype(bool)
        esc = df["ai_escalate_flag"].fillna(False).astype(bool)

        penalty = obs.astype(int) * 6 + esc.astype(int) * 10
        df["HI_final"] = np.clip(df["HI"] - penalty, 0, 100)
        df["Risk_final"] = df["HI_final"].apply(_risk_from_hi)
        df.loc[obs & (df["Risk_final"] == "ğŸŸ¢ å®‰å…¨"), "Risk_final"] = "ğŸŸ¡ é¢„è­¦"
        df.loc[esc, "Risk_final"] = "ğŸ”´ é«˜é£é™©"
    else:
        df["HI_final"] = df["HI"]
        df["Risk_final"] = df["Risk"]

    return df


def _calc_trigger_source(row: pd.Series):
    rule_hit = str(row.get("Risk_final", "")) == "ğŸ”´ é«˜é£é™©"
    ai_hit = bool(row.get("ai_escalate_flag", False))

    if rule_hit and ai_hit:
        return "both"
    if rule_hit:
        return "rule"
    if ai_hit:
        return "ai"
    return ""


def sync_alerts_from_scores(df_scored: pd.DataFrame):
    if df_scored is None or len(df_scored) == 0:
        return

    for _, row in df_scored.iterrows():
        source = _calc_trigger_source(row)
        if not source:
            continue

        create_or_update_alert(
            {
                "date": row["date"],
                "station": row["station"],
                "valve_type": row["valve_type"],
                "risk_level": row.get("Risk_final", "ğŸ”´ é«˜é£é™©"),
                "trigger_source": source,
                "trigger_detail": {
                    "HI_final": float(row.get("HI_final", np.nan)),
                    "ratio": float(row.get("ratio", np.nan)),
                    "ai_score_pct": float(row.get("ai_score_pct", np.nan)) if pd.notna(row.get("ai_score_pct", np.nan)) else None,
                    "ai_reason_top1": row.get("ai_reason_top1", "-"),
                    "ai_reason_top2": row.get("ai_reason_top2", "-"),
                    "ai_reason_top3": row.get("ai_reason_top3", "-"),
                },
            }
        )

# ================== UI ==================
st.title("åŸºäºIsolation Forestç®—æ³•çš„LNGå‚¨ç½å®‰å…¨é˜€å¥åº·ç›‘æµ‹ä¸é£é™©AIé¢„è­¦ç³»ç»Ÿ")
st.caption("åŒç«™ç‚¹åˆ†è§’è‰²ç®¡ç†ï½œæ»šåŠ¨60å¤©Isolation Forestï½œåŒæ¡ä»¶AIå‡çº§é¢„è­¦")

st.sidebar.divider()
st.sidebar.header("ğŸ§  AI å‚æ•°")
enable_ai = st.sidebar.checkbox("å¯ç”¨ Isolation Forest", value=True)
contamination = st.sidebar.slider("æ¨¡å‹å¼‚å¸¸æ¯”ä¾‹ï¼ˆè¶Šå¤§è¶Šæ•æ„Ÿï¼‰", min_value=0.02, max_value=0.20, value=0.08, step=0.01)
if enable_ai and not SKLEARN_OK:
    st.sidebar.warning("å½“å‰ç¯å¢ƒç¼ºå°‘ scikit-learnï¼ŒAIå¼‚å¸¸æ£€æµ‹ä¸å¯ç”¨ã€‚")
    enable_ai = False

if not IS_LEADER:
    st.sidebar.divider()
    st.sidebar.header("ğŸ“ æœ¬ç«™æ•°æ®å½•å…¥")
    st.sidebar.info(f"å½“å‰ç«™ç‚¹ï¼š{STATION_SCOPE}")

    valve_type = st.sidebar.selectbox("é€‰æ‹©å®‰å…¨é˜€ç±»å‹", ["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"])
    date = st.sidebar.date_input("æ—¥æœŸ")
    p_now = st.sidebar.number_input("å½“å‰å‹åŠ› p_now (MPa)", 0.0, 2.0, 1.20, 0.01)
    p_max = st.sidebar.number_input(
        "å½“æ—¥æœ€é«˜å‹åŠ› p_max (MPa)",
        0.0,
        2.0,
        1.20,
        0.01,
        help="å»ºè®®ï¼šp_max â‰¥ p_nowï¼›è‹¥è¾“å…¥å°äº p_nowï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰ p_now ä¿®æ­£ã€‚",
    )
    level = st.sidebar.number_input("æ¶²ä½ level (%)", 0, 100, 60)
    temp = st.sidebar.number_input("ç¯å¢ƒæ¸©åº¦ temp (â„ƒ)", -30, 60, 25)
    psv_act = st.sidebar.selectbox("æ˜¯å¦åŠ¨ä½œ", ["å¦", "æ˜¯"])
    psv_weeping = st.sidebar.selectbox("æ˜¯å¦å¾®æ”¾æ•£/å˜¶å˜¶å£°", ["å¦", "æ˜¯"])

    if st.sidebar.button("ä¿å­˜å¹¶è®¡ç®—", use_container_width=True):
        p_now_f = float(p_now)
        p_max_f = float(p_max)
        if p_max_f < p_now_f:
            st.sidebar.warning(f"å·²è‡ªåŠ¨ä¿®æ­£ï¼šp_max({p_max_f:.2f}) < p_now({p_now_f:.2f})ï¼Œå°† p_max è®¾ä¸º {p_now_f:.2f}")
            p_max_f = p_now_f

        try:
            save_record(
                {
                    "date": str(date),
                    "station": STATION_SCOPE,
                    "valve_type": valve_type,
                    "p_now": p_now_f,
                    "p_max": p_max_f,
                    "level": int(level),
                    "temp": int(temp),
                    "psv_act": 1 if psv_act == "æ˜¯" else 0,
                    "psv_weeping": 1 if psv_weeping == "æ˜¯" else 0,
                    "operator_role": ROLE,
                    "operator_name": st.session_state.user_name,
                },
                station_scope=STATION_SCOPE,
                role=ROLE,
            )
            st.sidebar.success("âœ… æ•°æ®å·²ä¿å­˜")
            st.rerun()
        except Exception as ex:
            st.sidebar.error(f"ä¿å­˜å¤±è´¥ï¼š{ex}")
else:
    st.sidebar.info("é¢†å¯¼è´¦å·ä¸ºåªè¯»æ¨¡å¼ï¼Œä¸å¯å½•å…¥æˆ–ä¿®æ”¹åŸå§‹æ•°æ®ã€‚")


# Load + score + auto alerts
df_raw = load_data(station_scope=STATION_SCOPE, role=ROLE)
df = compute_scores(df_raw, enable_ai=enable_ai, contamination=contamination)
sync_alerts_from_scores(df)
alerts = list_alerts(station_scope=STATION_SCOPE, role=ROLE)

if len(df) == 0:
    st.info("å½“å‰æƒé™èŒƒå›´å†…è¿˜æ²¡æœ‰æ•°æ®ã€‚")
    st.stop()


# Global date filter (default 30 days)
min_d, max_d = df["date"].min(), df["date"].max()
default_start = max(min_d, max_d - timedelta(days=29))
c1, c2, c3 = st.columns([1, 1, 2])
with c1:
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=default_start, min_value=min_d, max_value=max_d, key="global_start")
with c2:
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=max_d, min_value=min_d, max_value=max_d, key="global_end")
with c3:
    st.caption("é»˜è®¤çª—å£ï¼šè¿‘30å¤©ã€‚é¡µé¢é‡‡ç”¨åˆ†Tabç»“æ„ï¼Œé¿å…é•¿æ»šåŠ¨ã€‚")

df_f = df[(df["date"] >= start_date) & (df["date"] <= end_date)].copy()
alerts_f = alerts[
    (pd.to_datetime(alerts["date"], errors="coerce").dt.date >= start_date)
    & (pd.to_datetime(alerts["date"], errors="coerce").dt.date <= end_date)
].copy()

if len(df_f) == 0:
    st.warning("æ‰€é€‰æ—¥æœŸèŒƒå›´å†…æ— æ•°æ®ã€‚")
    st.stop()


def _slice_by_station(df_input: pd.DataFrame, station_pick: str) -> pd.DataFrame:
    if len(df_input) == 0:
        return df_input
    if station_pick == "å…¨éƒ¨ç«™ç‚¹":
        return df_input.copy()
    return df_input[df_input["station"] == station_pick].copy()


def build_hi_heatmap(df_filtered: pd.DataFrame) -> pd.DataFrame:
    if len(df_filtered) == 0:
        return pd.DataFrame()
    heat = df_filtered.pivot_table(index="valve_type", columns="date", values="HI_final", aggfunc="mean")
    return heat.sort_index()


def build_hi_compare(df_filtered: pd.DataFrame) -> pd.DataFrame:
    if len(df_filtered) == 0:
        return pd.DataFrame()
    return (
        df_filtered.groupby("valve_type")
        .agg(
            avg_HI=("HI_final", "mean"),
            min_HI=("HI_final", "min"),
            red_days=("Risk_final", lambda s: (s == "ğŸ”´ é«˜é£é™©").sum()),
            yellow_days=("Risk_final", lambda s: (s == "ğŸŸ¡ é¢„è­¦").sum()),
        )
        .reset_index()
        .sort_values("avg_HI")
    )


def build_pressure_trend(df_filtered: pd.DataFrame, station: str, valve: str) -> pd.DataFrame:
    sdf = _slice_by_station(df_filtered, station)
    if len(sdf) == 0:
        return sdf
    vdf = sdf[sdf["valve_type"] == valve].copy()
    if len(vdf) == 0:
        return vdf
    if station == "å…¨éƒ¨ç«™ç‚¹":
        vdf = (
            vdf.groupby("date", as_index=False)
            .agg(
                p_now=("p_now", "mean"),
                p_max=("p_max", "mean"),
                ai_observe_flag=("ai_observe_flag", "max"),
            )
            .sort_values("date")
        )
    else:
        vdf = vdf.sort_values("date")
    vdf["date_dt"] = pd.to_datetime(vdf["date"])
    return vdf


def build_leader_storyline(df_filtered: pd.DataFrame, alerts_filtered: pd.DataFrame) -> str:
    if len(df_filtered) == 0:
        return "å½“å‰èŒƒå›´æš‚æ— æ•°æ®ã€‚"

    comp = build_hi_compare(df_filtered)
    worst_name = "â€”"
    worst_hi = np.nan
    if len(comp) > 0:
        worst = comp.iloc[0]
        worst_name = str(worst["valve_type"])
        worst_hi = float(worst["avg_HI"])

    last_day = df_filtered["date"].max()
    recent7 = df_filtered[df_filtered["date"] >= (last_day - timedelta(days=6))]["HI_final"].mean()
    prev7 = df_filtered[
        (df_filtered["date"] >= (last_day - timedelta(days=13)))
        & (df_filtered["date"] <= (last_day - timedelta(days=7)))
    ]["HI_final"].mean()

    if np.isnan(recent7) or np.isnan(prev7):
        trend_text = "è¶‹åŠ¿æ ·æœ¬ä¸è¶³"
    else:
        delta = recent7 - prev7
        trend_text = f"è¿‘7å¤©è¾ƒå‰7å¤© {'ä¸Šå‡' if delta >= 0 else 'ä¸‹é™'} {abs(delta):.1f}"

    ai_obs = int(df_filtered["ai_observe_flag"].sum()) if "ai_observe_flag" in df_filtered.columns else 0
    ai_esc = int(df_filtered["ai_escalate_flag"].sum()) if "ai_escalate_flag" in df_filtered.columns else 0

    close_rate = 0.0
    if len(alerts_filtered) > 0:
        close_rate = float((alerts_filtered["status"] == "å·²å…³é—­").mean() * 100)

    return (
        f"é‡ç‚¹é˜€é—¨ï¼š{worst_name}ï¼ˆå¹³å‡HI {worst_hi:.1f}ï¼‰ï½œ{trend_text}ï½œ"
        f"AIè§‚å¯Ÿå¼‚å¸¸ {ai_obs} æ¬¡ï¼ŒAIå‡çº§é¢„è­¦ {ai_esc} æ¬¡ï½œå‘Šè­¦é—­ç¯ç‡ {close_rate:.1f}%"
    )

def render_tab_history(df_filtered: pd.DataFrame, alerts_filtered: pd.DataFrame):
    st.subheader("ğŸ“ˆ å†å²åˆ†æ")

    if IS_LEADER:
        station_pick = st.selectbox("ç«™ç‚¹", ["å…¨éƒ¨ç«™ç‚¹"] + sorted(df_filtered["station"].unique()), key="hist_station")
    else:
        station_pick = STATION_SCOPE
        st.info(f"å½“å‰ç«™ç‚¹ï¼š{station_pick}")

    hist_df = _slice_by_station(df_filtered, station_pick)
    hist_alerts = _slice_by_station(alerts_filtered, station_pick)

    if len(hist_df) == 0:
        st.warning("è¯¥èŒƒå›´å†…æš‚æ— æ•°æ®ã€‚")
        return

    valve_opts = sorted(hist_df["valve_type"].unique())
    valve_pick = st.selectbox("é˜€é—¨", valve_opts, key="hist_valve")
    vdf = build_pressure_trend(hist_df, "å…¨éƒ¨ç«™ç‚¹" if station_pick == "å…¨éƒ¨ç«™ç‚¹" else station_pick, valve_pick)

    c1, c2, c3 = st.columns(3, gap="small")

    with c1:
        st.markdown("**3çº¿å‹åŠ›è¶‹åŠ¿å›¾**")
        if len(vdf) == 0:
            st.info("æš‚æ— å¯ç»˜åˆ¶æ•°æ®")
        else:
            fig, ax = plt.subplots(figsize=(4.8, 3.1))
            ax.plot(vdf["date_dt"], vdf["p_now"], marker="o", linestyle="--", label="p_now")
            ax.plot(vdf["date_dt"], vdf["p_max"], marker="o", label="p_max")
            ax.axhline(SET_P, linestyle="-.", color="#6d4c41", label=f"æ•´å®šçº¿ {SET_P:.2f}MPa")

            ai_points = vdf[vdf["ai_observe_flag"] == True]
            if len(ai_points) > 0:
                ax.scatter(ai_points["date_dt"], ai_points["p_max"], color="#d32f2f", zorder=4, label="AIå¼‚å¸¸ç‚¹")

            ax.set_ylabel("MPa")
            ax.set_title("Pressure Trend (3 lines)")
            ax.xaxis.set_major_locator(mdates.AutoDateLocator())
            ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
            plt.xticks(rotation=30)
            ax.legend(fontsize=8)
            fig.tight_layout()
            st.pyplot(fig, use_container_width=True)

    with c2:
        st.markdown("**HIçƒ­åŠ›å›¾**")
        heat = build_hi_heatmap(hist_df)
        if len(heat) == 0:
            st.info("æš‚æ— å¯ç»˜åˆ¶æ•°æ®")
        else:
            fig, ax = plt.subplots(figsize=(4.8, 3.1))
            im = ax.imshow(heat.values, aspect="auto", cmap="RdYlGn", vmin=0, vmax=100)
            ax.set_yticks(range(len(heat.index)))
            ax.set_yticklabels(list(heat.index))

            cols = list(heat.columns)
            if len(cols) <= 8:
                tick_idx = list(range(len(cols)))
            else:
                tick_idx = sorted(set(np.linspace(0, len(cols) - 1, 7).round().astype(int).tolist()))
            ax.set_xticks(tick_idx)
            ax.set_xticklabels([pd.to_datetime(cols[i]).strftime("%m-%d") for i in tick_idx], rotation=30, ha="right")

            ax.set_title("HI Heatmap")
            plt.colorbar(im, ax=ax, fraction=0.045, pad=0.04)
            fig.tight_layout()
            st.pyplot(fig, use_container_width=True)

    with c3:
        st.markdown("**é˜€é—¨HIå¯¹æ¯”**")
        comp = build_hi_compare(hist_df)
        if len(comp) == 0:
            st.info("æš‚æ— å¯ç»˜åˆ¶æ•°æ®")
        else:
            fig, ax = plt.subplots(figsize=(4.8, 3.1))
            ax.bar(comp["valve_type"], comp["avg_HI"], color="#2e7d32")
            ax.set_ylim(0, 100)
            ax.set_ylabel("avg HI")
            ax.set_title("Valve HI Compare")
            plt.xticks(rotation=20, ha="right")
            fig.tight_layout()
            st.pyplot(fig, use_container_width=True)
            st.dataframe(comp[["valve_type", "avg_HI", "min_HI", "red_days", "yellow_days"]], use_container_width=True, height=170)

    st.info(build_leader_storyline(hist_df, hist_alerts))


def render_tab_ai(df_filtered: pd.DataFrame):
    st.subheader("ğŸ¤– AIé¢„è­¦ä¸­å¿ƒï¼ˆIsolation Forestï¼‰")
    st.caption("æ¨¡å‹å£å¾„ï¼šæŒ‰ station+valve_type åˆ†ç»„ã€æ»šåŠ¨60å¤©è®­ç»ƒã€æœ€å°æ ·æœ¬30ã€åŒæ¡ä»¶å‡çº§")

    if IS_LEADER:
        station_pick = st.selectbox("ç«™ç‚¹", ["å…¨éƒ¨ç«™ç‚¹"] + sorted(df_filtered["station"].unique()), key="ai_station")
    else:
        station_pick = STATION_SCOPE

    ai_df = _slice_by_station(df_filtered, station_pick)
    if len(ai_df) == 0:
        st.warning("è¯¥èŒƒå›´å†…æš‚æ— æ•°æ®ã€‚")
        return

    valve_opts = ["å…¨éƒ¨é˜€é—¨"] + sorted(ai_df["valve_type"].unique())
    valve_pick = st.selectbox("é˜€é—¨", valve_opts, key="ai_valve")
    if valve_pick != "å…¨éƒ¨é˜€é—¨":
        ai_df = ai_df[ai_df["valve_type"] == valve_pick].copy()

    m1, m2, m3 = st.columns(3)
    m1.metric("AIè§‚å¯Ÿå¼‚å¸¸æ•°", int(ai_df["ai_observe_flag"].sum()))
    m2.metric("AIå‡çº§é¢„è­¦æ•°", int(ai_df["ai_escalate_flag"].sum()))
    m3.metric("å¹³å‡AIåˆ†ä½", f"{ai_df['ai_score_pct'].mean():.1f}" if ai_df["ai_score_pct"].notna().any() else "â€”")

    trend = (
        ai_df.groupby("date")
        .agg(observe_cnt=("ai_observe_flag", "sum"), escalate_cnt=("ai_escalate_flag", "sum"))
        .reset_index()
        .sort_values("date")
    )
    trend["date_dt"] = pd.to_datetime(trend["date"])

    fig, ax = plt.subplots(figsize=(7.4, 3.0))
    ax.plot(trend["date_dt"], trend["observe_cnt"], marker="o", label="AIè§‚å¯Ÿå¼‚å¸¸")
    ax.plot(trend["date_dt"], trend["escalate_cnt"], marker="o", label="AIå‡çº§é¢„è­¦")
    ax.set_title("è¿‘30å¤©AIå¼‚å¸¸è¶‹åŠ¿")
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
    plt.xticks(rotation=30)
    ax.legend()
    fig.tight_layout()
    st.pyplot(fig, use_container_width=True)

    ai_list = ai_df[ai_df["ai_observe_flag"] == True].copy()
    if len(ai_list) == 0:
        st.info("å½“å‰èŒƒå›´æš‚æ— AIè§‚å¯Ÿå¼‚å¸¸ã€‚è‹¥æ¯ç»„æ ·æœ¬<30ï¼Œæ¨¡å‹ä»…åšè§„åˆ™é£é™©å±•ç¤ºï¼Œä¸è§¦å‘AIå‡çº§ã€‚")
    else:
        show_cols = [
            "date",
            "station",
            "valve_type",
            "ai_raw_score",
            "ai_score_pct",
            "ai_observe_flag",
            "ai_escalate_flag",
            "ai_reason_top1",
            "ai_reason_top2",
            "ai_reason_top3",
            "Risk",
            "Risk_final",
        ]
        st.dataframe(ai_list.sort_values("date", ascending=False)[show_cols], use_container_width=True)

def render_tab_dashboard(df_filtered: pd.DataFrame, alerts_filtered: pd.DataFrame, role: str):
    st.subheader("ğŸ“Š é©¾é©¶èˆ±")

    if IS_LEADER:
        station_pick = st.selectbox("ç«™ç‚¹", ["å…¨éƒ¨ç«™ç‚¹"] + sorted(df_filtered["station"].unique()), key="dash_station")
    else:
        station_pick = STATION_SCOPE

    dash_df = _slice_by_station(df_filtered, station_pick)
    dash_alerts = _slice_by_station(alerts_filtered, station_pick)

    if len(dash_df) == 0:
        st.warning("è¯¥èŒƒå›´å†…æš‚æ— æ•°æ®ã€‚")
        return

    today = dash_df["date"].max()
    week_start = today - timedelta(days=6)
    prev_week_start = today - timedelta(days=13)
    prev_week_end = today - timedelta(days=7)

    today_high_risk = int(((dash_df["date"] == today) & (dash_df["Risk_final"] == "ğŸ”´ é«˜é£é™©")).sum())
    alerts_week = dash_alerts[pd.to_datetime(dash_alerts["date"], errors="coerce").dt.date >= week_start]
    new_alerts_week = len(alerts_week)
    closed_week = int((alerts_week["status"] == "å·²å…³é—­").sum()) if len(alerts_week) else 0
    close_rate = (closed_week / new_alerts_week * 100) if new_alerts_week else 0

    cur_week_hi = dash_df[dash_df["date"] >= week_start]["HI_final"].mean()
    prev_week_hi = dash_df[(dash_df["date"] >= prev_week_start) & (dash_df["date"] <= prev_week_end)]["HI_final"].mean()
    hi_delta = 0 if np.isnan(cur_week_hi) or np.isnan(prev_week_hi) else cur_week_hi - prev_week_hi

    k1, k2, k3, k4 = st.columns(4)
    k1.metric("å½“æ—¥é«˜é£é™©é˜€é—¨æ•°", today_high_risk)
    k2.metric("æœ¬å‘¨æ–°å¢å‘Šè­¦æ•°", new_alerts_week)
    k3.metric("æœ¬å‘¨é—­ç¯ç‡", f"{close_rate:.1f}%")
    k4.metric("å¹³å‡HIè¾ƒä¸Šå‘¨", f"{hi_delta:+.1f}")

    left, right = st.columns([3, 2], gap="small")
    with left:
        if IS_LEADER and station_pick == "å…¨éƒ¨ç«™ç‚¹":
            comp = (
                df_filtered.groupby("station")
                .agg(
                    avg_HI=("HI_final", "mean"),
                    red_days=("Risk_final", lambda s: (s == "ğŸ”´ é«˜é£é™©").sum()),
                    yellow_days=("Risk_final", lambda s: (s == "ğŸŸ¡ é¢„è­¦").sum()),
                    activity=("Activity", "sum"),
                )
                .reindex(STATIONS)
                .fillna(0)
                .reset_index()
            )
            a2 = alerts_filtered.copy()
            if len(a2) > 0:
                a2["created_dt"] = pd.to_datetime(a2["created_at"], errors="coerce")
                a2["closed_dt"] = pd.to_datetime(a2["closed_at"], errors="coerce")
                a2["close_hours"] = (a2["closed_dt"] - a2["created_dt"]).dt.total_seconds() / 3600
                close_eff = a2.groupby("station")["close_hours"].mean().reindex(STATIONS)
                comp["å¹³å‡é—­ç¯æ—¶æ•ˆ(h)"] = comp["station"].map(close_eff).fillna(0).round(1)
            else:
                comp["å¹³å‡é—­ç¯æ—¶æ•ˆ(h)"] = 0
            st.dataframe(comp, use_container_width=True)
        else:
            latest = dash_df.sort_values(["valve_type", "date"]).groupby("valve_type").tail(1)
            st.dataframe(
                latest[["date", "station", "valve_type", "HI_final", "Risk_final", "ai_score_pct", "ai_escalate_flag"]],
                use_container_width=True,
            )

    with right:
        daily = (
            dash_df.groupby("date")
            .agg(avg_hi=("HI_final", "mean"), red_cnt=("Risk_final", lambda s: (s == "ğŸ”´ é«˜é£é™©").sum()))
            .reset_index()
            .sort_values("date")
        )
        daily["date_dt"] = pd.to_datetime(daily["date"])
        fig, ax = plt.subplots(figsize=(4.2, 3.0))
        ax.plot(daily["date_dt"], daily["avg_hi"], marker="o", label="å¹³å‡HI")
        ax.set_ylim(0, 100)
        ax2 = ax.twinx()
        ax2.bar(daily["date_dt"], daily["red_cnt"], alpha=0.2, color="#d32f2f", label="é«˜é£é™©æ•°")
        ax.set_title("HIä¸é«˜é£é™©è¶‹åŠ¿")
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
        plt.xticks(rotation=30)
        fig.tight_layout()
        st.pyplot(fig, use_container_width=True)


def render_tab_alerts(alerts_filtered: pd.DataFrame):
    st.subheader("ğŸš¨ å‘Šè­¦é—­ç¯")

    if IS_LEADER:
        station_pick = st.selectbox(
            "ç«™ç‚¹",
            ["å…¨éƒ¨ç«™ç‚¹"] + sorted(alerts_filtered["station"].unique()) if len(alerts_filtered) > 0 else ["å…¨éƒ¨ç«™ç‚¹"],
            key="alert_station",
        )
    else:
        station_pick = STATION_SCOPE

    view_alerts = _slice_by_station(alerts_filtered, station_pick)

    if len(view_alerts) == 0:
        st.info("å½“å‰æ—¥æœŸèŒƒå›´æ— å‘Šè­¦ã€‚")
        return

    show_cols = [
        "id",
        "date",
        "station",
        "valve_type",
        "risk_level",
        "trigger_source",
        "status",
        "owner",
        "action_taken",
        "verification_result",
        "updated_at",
        "closed_at",
    ]
    st.dataframe(view_alerts[show_cols], use_container_width=True)

    if IS_LEADER:
        st.info("é¢†å¯¼è´¦å·ä¸ºåªè¯»ï¼Œä¸å¯ä¿®æ”¹å‘Šè­¦çŠ¶æ€ã€‚")
        return

    work_alerts = view_alerts.copy()
    selected = st.selectbox("é€‰æ‹©å‘Šè­¦ID", work_alerts["id"].astype(str).tolist(), key="alert_id")
    row = work_alerts[work_alerts["id"].astype(str) == str(selected)].iloc[0]

    cur_status = row["status"] if row["status"] in STATUS_FLOW else "å¾…ç¡®è®¤"
    cur_i = STATUS_FLOW.index(cur_status)
    next_options = STATUS_FLOW[cur_i : min(cur_i + 2, len(STATUS_FLOW))]

    cc1, cc2 = st.columns(2)
    with cc1:
        st.text_input("å½“å‰çŠ¶æ€", value=cur_status, disabled=True, key="alert_cur")
    with cc2:
        new_status = st.selectbox("ç›®æ ‡çŠ¶æ€", next_options, index=0, key="alert_new")

    action_taken = st.text_area("æ•´æ”¹æªæ–½ï¼ˆå…³é—­å‰å¿…å¡«ï¼‰", value=str(row.get("action_taken", "")), key="alert_action")
    verification_result = st.text_area("å¤éªŒç»“æœï¼ˆå…³é—­å‰å¿…å¡«ï¼‰", value=str(row.get("verification_result", "")), key="alert_verify")

    if st.button("æ›´æ–°å‘Šè­¦çŠ¶æ€", use_container_width=True):
        try:
            update_alert_status(
                alert_id=str(selected),
                new_status=new_status,
                operator=st.session_state.user_name,
                action_taken=action_taken,
                verification_result=verification_result,
            )
            st.success("å‘Šè­¦çŠ¶æ€å·²æ›´æ–°")
            st.rerun()
        except Exception as ex:
            st.error(f"æ›´æ–°å¤±è´¥ï¼š{ex}")


def render_tab_reports(df_filtered: pd.DataFrame, alerts_filtered: pd.DataFrame):
    st.subheader("ğŸ“¥ æŠ¥è¡¨å¯¼å‡º")

    if IS_LEADER:
        station_pick = st.selectbox("ç«™ç‚¹", ["å…¨éƒ¨ç«™ç‚¹"] + sorted(df_filtered["station"].unique()), key="report_station")
    else:
        station_pick = STATION_SCOPE

    rep_df = _slice_by_station(df_filtered, station_pick)
    rep_alerts = _slice_by_station(alerts_filtered, station_pick)

    if len(rep_df) == 0:
        st.warning("è¯¥èŒƒå›´å†…æš‚æ— å¯å¯¼å‡ºæ•°æ®ã€‚")
        return

    exp1, exp2, exp3 = st.columns([1, 1, 1], gap="small")

    with exp1:
        csv_data = rep_df.sort_values(["station", "valve_type", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button(
            "ä¸‹è½½ç›‘æµ‹æ•°æ®CSV",
            data=csv_data,
            file_name="psv_data_filtered.csv",
            mime="text/csv",
            use_container_width=True,
        )

    with exp2:
        csv_alert = rep_alerts.sort_values(["station", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button(
            "ä¸‹è½½å‘Šè­¦æ•°æ®CSV",
            data=csv_alert,
            file_name="psv_alerts_filtered.csv",
            mime="text/csv",
            use_container_width=True,
        )

    with exp3:
        ai_rows = rep_df[rep_df["ai_observe_flag"] == True].copy()
        ai_csv = ai_rows[
            [
                "date",
                "station",
                "valve_type",
                "ai_raw_score",
                "ai_score_pct",
                "ai_observe_flag",
                "ai_escalate_flag",
                "ai_reason_top1",
                "ai_reason_top2",
                "ai_reason_top3",
            ]
        ].to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
        st.download_button(
            "ä¸‹è½½AIå¼‚å¸¸æ˜ç»†CSV",
            data=ai_csv,
            file_name="psv_ai_anomalies.csv",
            mime="text/csv",
            use_container_width=True,
        )

    avg_hi = rep_df["HI_final"].mean()
    red_cnt = int((rep_df["Risk_final"] == "ğŸ”´ é«˜é£é™©").sum())
    yellow_cnt = int((rep_df["Risk_final"] == "ğŸŸ¡ é¢„è­¦").sum())
    ai_obs = int(rep_df["ai_observe_flag"].sum())
    ai_esc = int(rep_df["ai_escalate_flag"].sum())
    close_rate = float((rep_alerts["status"] == "å·²å…³é—­").mean() * 100) if len(rep_alerts) else 0.0

    top_reasons = (
        rep_df.loc[rep_df["ai_observe_flag"] == True, "ai_reason_top1"]
        .value_counts()
        .head(3)
        .to_dict()
    )
    top_reason_text = "ã€".join([f"{k}:{v}æ¬¡" for k, v in top_reasons.items()]) if top_reasons else "æš‚æ— "

    summary_lines = [
        "é¡¹ç›®åç§°ï¼šåŸºäºIsolation Forestç®—æ³•çš„LNGå‚¨ç½å®‰å…¨é˜€å¥åº·ç›‘æµ‹ä¸é£é™©AIé¢„è­¦ç³»ç»Ÿ",
        f"æŠ¥å‘ŠèŒƒå›´ï¼š{start_date} è‡³ {end_date}",
        f"è´¦å·èŒƒå›´ï¼š{station_pick}",
        f"å¹³å‡HIï¼š{avg_hi:.1f}",
        f"é«˜é£é™©è®°å½•æ•°ï¼š{red_cnt}",
        f"é¢„è­¦è®°å½•æ•°ï¼š{yellow_cnt}",
        f"AIè§‚å¯Ÿå¼‚å¸¸æ•°ï¼š{ai_obs}",
        f"AIå‡çº§é¢„è­¦æ•°ï¼š{ai_esc}",
        f"AIå¼‚å¸¸ä¸»å› Top3ï¼š{top_reason_text}",
        f"å‘Šè­¦é—­ç¯ç‡ï¼š{close_rate:.1f}%",
        build_leader_storyline(rep_df, rep_alerts),
        "å»ºè®®ï¼šä¼˜å…ˆå¤„ç†AIå‡çº§é¢„è­¦ä¸”è¿ç»­2å¤©å¼‚å¸¸çš„é˜€é—¨ï¼Œå¤éªŒåå…³é—­å·¥å•ã€‚",
    ]

    summary_text = "\n".join(summary_lines)
    st.text_area("ä¸€é”®å‘¨æŠ¥æ‘˜è¦ï¼ˆå¯ç›´æ¥è´´PPTï¼‰", value=summary_text, height=220)
    st.download_button(
        "ä¸‹è½½ç®¡ç†æ‘˜è¦TXT",
        data=summary_text.encode("utf-8"),
        file_name="management_summary.txt",
        mime="text/plain",
        use_container_width=True,
    )

    st.markdown("**æœ€è¿‘20æ¡è®°å½•**")
    show_cols = [
        "date",
        "station",
        "valve_type",
        "p_now",
        "p_max",
        "level",
        "temp",
        "HI_final",
        "Risk_final",
        "ai_score_pct",
        "ai_observe_flag",
        "ai_escalate_flag",
    ]
    st.dataframe(rep_df.sort_values("date", ascending=False)[show_cols].head(20), use_container_width=True)


# ================== Top Tabs ==================
# é»˜è®¤é¦–å¼€ç¬¬ä¸€ä¸ªTabï¼šå†å²åˆ†æ
hist_tab, ai_tab, dash_tab, alert_tab, report_tab = st.tabs(
    ["å†å²åˆ†æ", "AIé¢„è­¦ä¸­å¿ƒ", "é©¾é©¶èˆ±", "å‘Šè­¦é—­ç¯", "æŠ¥è¡¨å¯¼å‡º"]
)

with hist_tab:
    render_tab_history(df_f, alerts_f)

with ai_tab:
    render_tab_ai(df_f)

with dash_tab:
    render_tab_dashboard(df_f, alerts_f, ROLE)

with alert_tab:
    render_tab_alerts(alerts_f)

with report_tab:
    render_tab_reports(df_f, alerts_f)
