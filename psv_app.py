import os
import uuid
from datetime import timedelta

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import matplotlib as mpl
from matplotlib import font_manager


# ================== Config ==================
SET_P = 1.32
DATA_FILE = "psv_data.csv"
ALERT_FILE = "psv_alerts.csv"
AUDIT_FILE = "psv_audit_logs.csv"

TABLE_DATA = "psv_data"
TABLE_ALERT = "psv_alerts"
TABLE_AUDIT = "psv_audit_logs"

STATUS_FLOW = ["å¾…ç¡®è®¤", "å·²æ´¾å·¥", "å¤„ç†ä¸­", "å·²éªŒè¯", "å·²å…³é—­"]
STATIONS = ["åç›˜LNGåŠ æ°”ç«™", "ç½—æ‰€LNGåŠ æ°”ç«™"]
DEFAULT_STATION = "åç›˜LNGåŠ æ°”ç«™"


# ================== Supabase init ==================
try:
    from supabase import create_client

    SUPABASE_OK = True
except Exception:
    SUPABASE_OK = False


def _secret_get(key: str, default=""):
    try:
        return st.secrets.get(key, default)
    except Exception:
        return default


SUPABASE_URL = _secret_get("SUPABASE_URL", "https://ynowvxcsvjskwkeauvkz.supabase.co") or os.getenv("SUPABASE_URL", "https://ynowvxcsvjskwkeauvkz.supabase.co")
SUPABASE_KEY = _secret_get("SUPABASE_KEY", "sb_publishable_aezshZPqB78WBtyWtTf8Tg_UVpCEZzd") or os.getenv("SUPABASE_KEY", "sb_publishable_aezshZPqB78WBtyWtTf8Tg_UVpCEZzd")

USE_SUPABASE = True
supabase = None
if USE_SUPABASE and SUPABASE_OK and SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
elif USE_SUPABASE:
    st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ° Supabase é…ç½®ï¼ˆSUPABASE_URL / SUPABASE_KEYï¼‰ï¼Œå°†å›é€€ä¸ºæœ¬åœ°CSVå­˜å‚¨ã€‚")
    USE_SUPABASE = False


# ================== ML ==================
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler

    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False


# ================== Font ==================
def _setup_cjk_font():
    preferred_font_names = [
        "Noto Sans CJK SC",
        "Noto Sans CJK JP",
        "Noto Sans CJK TC",
        "Microsoft YaHei",
        "SimHei",
        "PingFang SC",
        "WenQuanYi Micro Hei",
        "Source Han Sans SC",
    ]

    available = {f.name for f in font_manager.fontManager.ttflist}
    for name in preferred_font_names:
        if name in available:
            mpl.rcParams["font.family"] = "sans-serif"
            mpl.rcParams["font.sans-serif"] = [name, "DejaVu Sans"]
            mpl.rcParams["axes.unicode_minus"] = False
            return

    font_path = "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc"
    if os.path.exists(font_path):
        fp = font_manager.FontProperties(fname=font_path)
        mpl.rcParams["font.family"] = fp.get_name()
        mpl.rcParams["axes.unicode_minus"] = False


_setup_cjk_font()
st.set_page_config(page_title="LNGå®‰å…¨é˜€å¤šç«™ç‚¹ç›‘æµ‹ç³»ç»Ÿ", layout="wide")


# ================== Auth ==================
def load_accounts() -> dict:
    hp = _secret_get("PASSWORD_HUAPAN", "hp123456") or os.getenv("PASSWORD_HUAPAN", "hp123456")
    ls = _secret_get("PASSWORD_LUOSUO", "ls123456") or os.getenv("PASSWORD_LUOSUO", "ls123456")
    leader = _secret_get("PASSWORD_LEADER", "leader123456") or os.getenv("PASSWORD_LEADER", "leader123456")

    return {
        "åç›˜ç«™": {"password": hp, "role": "station", "station_scope": "åç›˜LNGåŠ æ°”ç«™"},
        "ç½—æ‰€ç«™": {"password": ls, "role": "station", "station_scope": "ç½—æ‰€LNGåŠ æ°”ç«™"},
        "é¢†å¯¼": {"password": leader, "role": "leader", "station_scope": "ALL"},
    }


ACCOUNTS = load_accounts()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False
if "user_name" not in st.session_state:
    st.session_state.user_name = ""
if "role" not in st.session_state:
    st.session_state.role = ""
if "station_scope" not in st.session_state:
    st.session_state.station_scope = ""

st.sidebar.title("ğŸ” è®¿é—®æ§åˆ¶")

if not st.session_state.authenticated:
    login_name = st.sidebar.selectbox("è´¦å·", list(ACCOUNTS.keys()), index=0)
    login_pwd = st.sidebar.text_input("å¯†ç ", type="password")
    if st.sidebar.button("ç™»å½•", use_container_width=True):
        if login_pwd == ACCOUNTS[login_name]["password"]:
            st.session_state.authenticated = True
            st.session_state.user_name = login_name
            st.session_state.role = ACCOUNTS[login_name]["role"]
            st.session_state.station_scope = ACCOUNTS[login_name]["station_scope"]
            st.rerun()
        else:
            st.sidebar.error("å¯†ç é”™è¯¯")
    st.warning("è¯·è¾“å…¥è´¦å·å’Œå¯†ç åè¿›å…¥ç³»ç»Ÿã€‚")
    st.stop()
else:
    st.sidebar.success(
        f"å·²ç™»å½•ï¼š{st.session_state.user_name} | è§’è‰²ï¼š{st.session_state.role} | èŒƒå›´ï¼š{st.session_state.station_scope}"
    )
    if st.sidebar.button("é€€å‡ºç™»å½•", use_container_width=True):
        for k in ["authenticated", "user_name", "role", "station_scope"]:
            st.session_state.pop(k, None)
        st.rerun()


ROLE = st.session_state.role
STATION_SCOPE = st.session_state.station_scope
IS_LEADER = ROLE == "leader"


# ================== Data helpers ==================
BASE_DATA_COLS = [
    "date",
    "station",
    "valve_type",
    "p_now",
    "p_max",
    "level",
    "temp",
    "psv_act",
    "psv_weeping",
    "operator_role",
    "operator_name",
    "updated_at",
]

BASE_ALERT_COLS = [
    "id",
    "date",
    "station",
    "valve_type",
    "risk_level",
    "trigger_source",
    "trigger_detail",
    "status",
    "owner",
    "action_taken",
    "verification_result",
    "created_at",
    "updated_at",
    "closed_at",
]

BASE_AUDIT_COLS = ["id", "entity_type", "entity_id", "action", "operator", "payload", "created_at"]


def _ensure_local_files():
    if not os.path.exists(DATA_FILE):
        pd.DataFrame(columns=BASE_DATA_COLS).to_csv(DATA_FILE, index=False, encoding="utf-8-sig")
    if not os.path.exists(ALERT_FILE):
        pd.DataFrame(columns=BASE_ALERT_COLS).to_csv(ALERT_FILE, index=False, encoding="utf-8-sig")
    if not os.path.exists(AUDIT_FILE):
        pd.DataFrame(columns=BASE_AUDIT_COLS).to_csv(AUDIT_FILE, index=False, encoding="utf-8-sig")


_ensure_local_files()


def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or len(df) == 0:
        return df

    rename_map = {
        "Valve_type": "valve_type",
        "P_now": "p_now",
        "P_max": "p_max",
        "Level": "level",
        "Temp": "temp",
        "PSV_act": "psv_act",
        "PSV_weeping": "psv_weeping",
    }
    df = df.rename(columns=rename_map)

    for c in BASE_DATA_COLS:
        if c not in df.columns:
            if c == "station":
                df[c] = DEFAULT_STATION
            elif c in ["operator_role", "operator_name", "updated_at"]:
                df[c] = ""
            else:
                df[c] = np.nan

    return df[BASE_DATA_COLS]


def _normalize_df(df0: pd.DataFrame) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return pd.DataFrame(columns=BASE_DATA_COLS)

    df = _standardize_columns(df0.copy())
    df["station"] = df["station"].fillna(DEFAULT_STATION).replace("", DEFAULT_STATION)
    df["station"] = df["station"].where(df["station"].isin(STATIONS), DEFAULT_STATION)

    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date

    for col in ["p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df["p_now"] = df["p_now"].clip(lower=0, upper=2)
    df["p_max"] = df["p_max"].clip(lower=0, upper=2)
    df["level"] = df["level"].clip(lower=0, upper=100)
    df["temp"] = df["temp"].clip(lower=-50, upper=80)

    m = df["p_max"].notna() & df["p_now"].notna() & (df["p_max"] < df["p_now"])
    if m.any():
        df.loc[m, "p_max"] = df.loc[m, "p_now"]

    df = df.dropna(subset=["date", "station", "valve_type", "p_max"])
    df = (
        df.sort_values(["station", "valve_type", "date", "updated_at"])
        .drop_duplicates(subset=["date", "station", "valve_type"], keep="last")
        .reset_index(drop=True)
    )

    return df


def _scope_filter(df: pd.DataFrame, station_scope: str) -> pd.DataFrame:
    if len(df) == 0:
        return df
    if station_scope == "ALL":
        return df
    return df[df["station"] == station_scope].copy()


def load_data(station_scope: str, role: str) -> pd.DataFrame:
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table(TABLE_DATA).select("*").execute()
        raw = pd.DataFrame(resp.data or [])
    else:
        raw = pd.read_csv(DATA_FILE)

    df = _normalize_df(raw)
    return _scope_filter(df, station_scope)


def _write_local_data(df: pd.DataFrame):
    df.to_csv(DATA_FILE, index=False, encoding="utf-8-sig")


def save_record(record: dict, station_scope: str, role: str) -> None:
    if role == "leader":
        raise PermissionError("é¢†å¯¼è´¦å·ä¸ºåªè¯»ï¼Œä¸å…è®¸å†™å…¥æ•°æ®")

    if station_scope != "ALL" and record.get("station") != station_scope:
        raise PermissionError("åªèƒ½å†™å…¥æœ¬ç«™æ•°æ®")

    record = record.copy()
    record["updated_at"] = pd.Timestamp.now().isoformat()

    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_DATA).upsert(record, on_conflict="date,station,valve_type").execute()
        return

    df_all = _normalize_df(pd.read_csv(DATA_FILE))
    new_row = pd.DataFrame([record])
    merged = pd.concat([df_all, new_row], ignore_index=True)
    merged = _normalize_df(merged)
    _write_local_data(merged)


# ================== Alerts ==================
def _normalize_alert_df(df0: pd.DataFrame) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return pd.DataFrame(columns=BASE_ALERT_COLS)

    df = df0.copy()
    for c in BASE_ALERT_COLS:
        if c not in df.columns:
            df[c] = ""

    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
    df["station"] = df["station"].fillna(DEFAULT_STATION).replace("", DEFAULT_STATION)
    df["status"] = df["status"].replace("", "å¾…ç¡®è®¤")
    return df[BASE_ALERT_COLS]


def _load_alerts_all() -> pd.DataFrame:
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table(TABLE_ALERT).select("*").execute()
        raw = pd.DataFrame(resp.data or [])
    else:
        raw = pd.read_csv(ALERT_FILE)
    return _normalize_alert_df(raw)


def _save_alerts_local(df: pd.DataFrame):
    df.to_csv(ALERT_FILE, index=False, encoding="utf-8-sig")


def append_audit(entity_type: str, entity_id: str, action: str, operator: str, payload: str):
    log = {
        "id": str(uuid.uuid4()),
        "entity_type": entity_type,
        "entity_id": str(entity_id),
        "action": action,
        "operator": operator,
        "payload": payload,
        "created_at": pd.Timestamp.now().isoformat(),
    }
    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_AUDIT).insert(log).execute()
    else:
        df = pd.read_csv(AUDIT_FILE)
        df = pd.concat([df, pd.DataFrame([log])], ignore_index=True)
        df.to_csv(AUDIT_FILE, index=False, encoding="utf-8-sig")


def _find_alert(alerts: pd.DataFrame, date_value, station: str, valve_type: str):
    m = (
        (alerts["date"] == pd.to_datetime(date_value).date())
        & (alerts["station"] == station)
        & (alerts["valve_type"] == valve_type)
    )
    return alerts[m]


def create_or_update_alert(record: dict):
    now_iso = pd.Timestamp.now().isoformat()
    alerts = _load_alerts_all()

    found = _find_alert(alerts, record["date"], record["station"], record["valve_type"])
    if len(found) > 0:
        idx = found.index[0]
        keep_status = alerts.loc[idx, "status"] or "å¾…ç¡®è®¤"
        alerts.loc[idx, "risk_level"] = record.get("risk_level", alerts.loc[idx, "risk_level"])
        alerts.loc[idx, "trigger_source"] = record.get("trigger_source", alerts.loc[idx, "trigger_source"])
        alerts.loc[idx, "trigger_detail"] = str(record.get("trigger_detail", alerts.loc[idx, "trigger_detail"]))
        alerts.loc[idx, "updated_at"] = now_iso
        alerts.loc[idx, "status"] = keep_status
    else:
        new_alert = {
            "id": str(uuid.uuid4()),
            "date": str(record["date"]),
            "station": record["station"],
            "valve_type": record["valve_type"],
            "risk_level": record.get("risk_level", "ğŸ”´ é«˜é£é™©"),
            "trigger_source": record.get("trigger_source", "rule"),
            "trigger_detail": str(record.get("trigger_detail", "")),
            "status": "å¾…ç¡®è®¤",
            "owner": "",
            "action_taken": "",
            "verification_result": "",
            "created_at": now_iso,
            "updated_at": now_iso,
            "closed_at": "",
        }
        alerts = pd.concat([alerts, pd.DataFrame([new_alert])], ignore_index=True)

    if USE_SUPABASE and supabase is not None:
        rows = alerts.to_dict(orient="records")
        supabase.table(TABLE_ALERT).upsert(rows, on_conflict="id").execute()
    else:
        _save_alerts_local(alerts)


def update_alert_status(alert_id: str, new_status: str, operator: str, action_taken: str = "", verification_result: str = ""):
    alerts = _load_alerts_all()
    hit = alerts[alerts["id"].astype(str) == str(alert_id)]
    if len(hit) == 0:
        raise ValueError("æœªæ‰¾åˆ°å‘Šè­¦")

    idx = hit.index[0]
    cur = alerts.loc[idx, "status"]
    if cur not in STATUS_FLOW:
        cur = "å¾…ç¡®è®¤"
    if new_status not in STATUS_FLOW:
        raise ValueError("éæ³•çŠ¶æ€")

    cur_i = STATUS_FLOW.index(cur)
    new_i = STATUS_FLOW.index(new_status)
    if not (new_i == cur_i or new_i == cur_i + 1):
        raise ValueError("çŠ¶æ€ä»…å…è®¸ä¿æŒä¸å˜æˆ–æ¨è¿›ä¸€æ­¥")

    if new_status == "å·²å…³é—­":
        if not action_taken.strip() or not verification_result.strip():
            raise ValueError("å…³é—­å‘Šè­¦å‰å¿…é¡»å¡«å†™æ•´æ”¹æªæ–½å’Œå¤éªŒç»“æœ")
        alerts.loc[idx, "closed_at"] = pd.Timestamp.now().isoformat()

    if action_taken.strip():
        alerts.loc[idx, "action_taken"] = action_taken.strip()
    if verification_result.strip():
        alerts.loc[idx, "verification_result"] = verification_result.strip()

    alerts.loc[idx, "status"] = new_status
    alerts.loc[idx, "owner"] = operator
    alerts.loc[idx, "updated_at"] = pd.Timestamp.now().isoformat()

    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_ALERT).upsert(alerts.to_dict(orient="records"), on_conflict="id").execute()
    else:
        _save_alerts_local(alerts)

    append_audit(
        entity_type="alert",
        entity_id=str(alert_id),
        action=f"status:{cur}->{new_status}",
        operator=operator,
        payload=f"action_taken={action_taken}; verification_result={verification_result}",
    )


def list_alerts(station_scope: str, role: str) -> pd.DataFrame:
    alerts = _load_alerts_all()
    if station_scope != "ALL":
        alerts = alerts[alerts["station"] == station_scope].copy()
    if len(alerts) == 0:
        return alerts
    return alerts.sort_values(["status", "date"], ascending=[True, False]).reset_index(drop=True)


# ================== Scoring ==================
def compute_scores(df0: pd.DataFrame, enable_ai: bool, contamination: float) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return df0

    df = _normalize_df(df0).copy()
    df = df.sort_values(["station", "valve_type", "date"]).reset_index(drop=True)

    df["ratio"] = df["p_max"] / SET_P
    df["slope"] = (
        df.groupby(["station", "valve_type"])["p_max"]
        .apply(lambda s: s.diff().rolling(3).mean())
        .reset_index(level=[0, 1], drop=True)
    )

    hi = np.full(len(df), 100.0)
    hi -= np.where(df["ratio"] >= 1.00, 35, 0)
    hi -= np.where((df["ratio"] >= 0.98) & (df["ratio"] < 1.00), 20, 0)
    hi -= np.where((df["ratio"] >= 0.95) & (df["ratio"] < 0.98), 10, 0)

    hi -= np.where(df["slope"] > 0.01, 10, 0)
    hi -= np.where(df["slope"] > 0.02, 10, 0)

    hi -= df.get("psv_act", 0).fillna(0) * 30
    hi -= df.get("psv_weeping", 0).fillna(0) * 15

    hi -= np.where(
        (df.get("temp", 0).fillna(0) >= 33)
        & (df.get("level", 0).fillna(0) >= 80)
        & (df["ratio"] >= 0.95),
        10,
        0,
    )

    df["HI"] = np.clip(hi, 0, 100)

    def risk(x: float) -> str:
        if x >= 85:
            return "ğŸŸ¢ å®‰å…¨"
        if x >= 70:
            return "ğŸŸ¡ é¢„è­¦"
        return "ğŸ”´ é«˜é£é™©"

    df["Risk"] = df["HI"].apply(risk)
    df["Activity"] = df.get("psv_act", 0).fillna(0) + df.get("psv_weeping", 0).fillna(0)

    df["AI_anomaly"] = False
    df["AI_score"] = np.nan

    if enable_ai and SKLEARN_OK:
        features = ["p_now", "p_max", "level", "temp", "ratio", "slope", "Activity"]
        for _, g in df.groupby(["station", "valve_type"]):
            idx = g.index
            if len(g) < 10:
                continue
            x = g[features].copy().apply(pd.to_numeric, errors="coerce")
            x = x.fillna(x.median(numeric_only=True))

            scaler = StandardScaler()
            xs = scaler.fit_transform(x.values)

            iso = IsolationForest(n_estimators=200, contamination=float(contamination), random_state=42)
            iso.fit(xs)
            pred = iso.predict(xs)
            score = -iso.score_samples(xs)

            df.loc[idx, "AI_anomaly"] = pred == -1
            df.loc[idx, "AI_score"] = score

        df["HI_final"] = np.clip(df["HI"] - df["AI_anomaly"].astype(int) * 10, 0, 100)
    else:
        df["HI_final"] = df["HI"]

    df["Risk_final"] = df["HI_final"].apply(risk)
    return df


def _calc_trigger_source(row: pd.Series):
    rule_hit = str(row.get("Risk_final", "")) == "ğŸ”´ é«˜é£é™©"
    ai_hit = bool(row.get("AI_anomaly", False))

    if rule_hit and ai_hit:
        return "both"
    if rule_hit:
        return "rule"
    if ai_hit:
        return "ai"
    return ""


def sync_alerts_from_scores(df_scored: pd.DataFrame):
    if df_scored is None or len(df_scored) == 0:
        return

    for _, row in df_scored.iterrows():
        source = _calc_trigger_source(row)
        if not source:
            continue
        create_or_update_alert(
            {
                "date": row["date"],
                "station": row["station"],
                "valve_type": row["valve_type"],
                "risk_level": row.get("Risk_final", "ğŸ”´ é«˜é£é™©"),
                "trigger_source": source,
                "trigger_detail": {
                    "HI_final": float(row.get("HI_final", np.nan)),
                    "ratio": float(row.get("ratio", np.nan)),
                    "AI_anomaly": bool(row.get("AI_anomaly", False)),
                },
            }
        )


# ================== UI ==================
st.title("LNGå®‰å…¨é˜€å¤šç«™ç‚¹AIå¥åº·ç›‘æµ‹ä¸å‘Šè­¦é—­ç¯ç³»ç»Ÿ")
st.caption("ä¸€æœŸï¼šåç›˜ç«™/ç½—æ‰€ç«™/é¢†å¯¼ä¸‰è´¦å·ï¼ŒæŒ‰ç«™ç‚¹æ•°æ®éš”ç¦»ï¼Œé¢†å¯¼åªè¯»ã€‚")

st.sidebar.divider()
st.sidebar.header("ğŸ§  AI å¼‚å¸¸æ£€æµ‹")
enable_ai = st.sidebar.checkbox("å¯ç”¨ AI å¼‚å¸¸æ£€æµ‹", value=True)
contamination = st.sidebar.slider("å¼‚å¸¸æ¯”ä¾‹ï¼ˆè¶Šå¤§è¶Šæ•æ„Ÿï¼‰", min_value=0.02, max_value=0.20, value=0.08, step=0.01)
if enable_ai and not SKLEARN_OK:
    st.sidebar.warning("å½“å‰ç¯å¢ƒç¼ºå°‘ scikit-learnï¼ŒAIå¼‚å¸¸æ£€æµ‹ä¸å¯ç”¨ã€‚")
    enable_ai = False

if not IS_LEADER:
    st.sidebar.divider()
    st.sidebar.header("ğŸ“ æœ¬ç«™æ•°æ®å½•å…¥")
    st.sidebar.info(f"å½“å‰ç«™ç‚¹ï¼š{STATION_SCOPE}")

    valve_type = st.sidebar.selectbox("é€‰æ‹©å®‰å…¨é˜€ç±»å‹", ["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"])
    date = st.sidebar.date_input("æ—¥æœŸ")
    p_now = st.sidebar.number_input("å½“å‰å‹åŠ› p_now (MPa)", 0.0, 2.0, 1.20, 0.01)
    p_max = st.sidebar.number_input(
        "å½“æ—¥æœ€é«˜å‹åŠ› p_max (MPa)",
        0.0,
        2.0,
        1.20,
        0.01,
        help="å»ºè®®ï¼šp_max â‰¥ p_nowï¼›è‹¥è¾“å…¥å°äº p_nowï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰ p_now ä¿®æ­£ã€‚",
    )
    level = st.sidebar.number_input("æ¶²ä½ level (%)", 0, 100, 60)
    temp = st.sidebar.number_input("ç¯å¢ƒæ¸©åº¦ temp (â„ƒ)", -30, 60, 25)
    psv_act = st.sidebar.selectbox("æ˜¯å¦åŠ¨ä½œ", ["å¦", "æ˜¯"])
    psv_weeping = st.sidebar.selectbox("æ˜¯å¦å¾®æ”¾æ•£/å˜¶å˜¶å£°", ["å¦", "æ˜¯"])

    if st.sidebar.button("ä¿å­˜å¹¶è®¡ç®—", use_container_width=True):
        p_now_f = float(p_now)
        p_max_f = float(p_max)
        if p_max_f < p_now_f:
            st.sidebar.warning(f"å·²è‡ªåŠ¨ä¿®æ­£ï¼šp_max({p_max_f:.2f}) < p_now({p_now_f:.2f})ï¼Œå°† p_max è®¾ä¸º {p_now_f:.2f}")
            p_max_f = p_now_f

        try:
            save_record(
                {
                    "date": str(date),
                    "station": STATION_SCOPE,
                    "valve_type": valve_type,
                    "p_now": p_now_f,
                    "p_max": p_max_f,
                    "level": int(level),
                    "temp": int(temp),
                    "psv_act": 1 if psv_act == "æ˜¯" else 0,
                    "psv_weeping": 1 if psv_weeping == "æ˜¯" else 0,
                    "operator_role": ROLE,
                    "operator_name": st.session_state.user_name,
                },
                station_scope=STATION_SCOPE,
                role=ROLE,
            )
            st.sidebar.success("âœ… æ•°æ®å·²ä¿å­˜")
            st.rerun()
        except Exception as ex:
            st.sidebar.error(f"ä¿å­˜å¤±è´¥ï¼š{ex}")
else:
    st.sidebar.info("é¢†å¯¼è´¦å·ä¸ºåªè¯»æ¨¡å¼ï¼Œä¸å¯å½•å…¥æˆ–ä¿®æ”¹åŸå§‹æ•°æ®ã€‚")


# Load + score + auto alerts
df_raw = load_data(station_scope=STATION_SCOPE, role=ROLE)
df = compute_scores(df_raw, enable_ai=enable_ai, contamination=contamination)
sync_alerts_from_scores(df)
alerts = list_alerts(station_scope=STATION_SCOPE, role=ROLE)

if len(df) == 0:
    st.info("å½“å‰æƒé™èŒƒå›´å†…è¿˜æ²¡æœ‰æ•°æ®ã€‚")
    st.stop()

# Common date filter
min_d, max_d = df["date"].min(), df["date"].max()
c1, c2, c3 = st.columns([1, 1, 2])
with c1:
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=min_d, min_value=min_d, max_value=max_d, key="start")
with c2:
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=max_d, min_value=min_d, max_value=max_d, key="end")
with c3:
    st.caption("å»ºè®®ï¼šæ±‡æŠ¥åœºæ™¯ä¼˜å…ˆé€‰æ‹©æœ€è¿‘30å¤©ã€‚")

df_f = df[(df["date"] >= start_date) & (df["date"] <= end_date)].copy()
alerts_f = alerts[
    (pd.to_datetime(alerts["date"], errors="coerce").dt.date >= start_date)
    & (pd.to_datetime(alerts["date"], errors="coerce").dt.date <= end_date)
].copy()

if IS_LEADER:
    tab_dashboard, tab_alert, tab_export, tab_history = st.tabs(["é¢†å¯¼é©¾é©¶èˆ±", "å‘Šè­¦ä¸­å¿ƒ", "æŠ¥è¡¨å¯¼å‡º", "å†å²åˆ†æ"])
else:
    tab_dashboard, tab_alert, tab_export, tab_history = st.tabs(["ç«™ç‚¹å·¥ä½œå°", "å‘Šè­¦ä¸­å¿ƒ", "æŠ¥è¡¨å¯¼å‡º", "å†å²åˆ†æ"])

with tab_dashboard:
    if IS_LEADER:
        st.subheader("ğŸ“Š é¢†å¯¼é©¾é©¶èˆ±")
        today = df_f["date"].max()
        week_start = today - timedelta(days=6)
        prev_week_start = today - timedelta(days=13)
        prev_week_end = today - timedelta(days=7)

        today_high_risk = int(((df_f["date"] == today) & (df_f["Risk_final"] == "ğŸ”´ é«˜é£é™©")).sum())
        alerts_week = alerts_f[pd.to_datetime(alerts_f["date"]).dt.date >= week_start]
        new_alerts_week = len(alerts_week)
        closed_week = int((alerts_week["status"] == "å·²å…³é—­").sum()) if len(alerts_week) else 0
        close_rate = (closed_week / new_alerts_week * 100) if new_alerts_week else 0

        cur_week_hi = df_f[(df_f["date"] >= week_start)]["HI_final"].mean()
        prev_week_hi = df_f[(df_f["date"] >= prev_week_start) & (df_f["date"] <= prev_week_end)]["HI_final"].mean()
        hi_delta = 0 if np.isnan(cur_week_hi) or np.isnan(prev_week_hi) else cur_week_hi - prev_week_hi

        k1, k2, k3, k4 = st.columns(4)
        k1.metric("å½“æ—¥é«˜é£é™©é˜€é—¨æ•°", today_high_risk)
        k2.metric("æœ¬å‘¨æ–°å¢å‘Šè­¦æ•°", new_alerts_week)
        k3.metric("æœ¬å‘¨é—­ç¯ç‡", f"{close_rate:.1f}%")
        k4.metric("å¹³å‡HIè¾ƒä¸Šå‘¨", f"{hi_delta:+.1f}")

        st.markdown("**åç›˜ vs ç½—æ‰€ å¯¹æ¯”**")
        comp = (
            df_f.groupby("station")
            .agg(
                avg_HI=("HI_final", "mean"),
                red_days=("Risk_final", lambda s: (s == "ğŸ”´ é«˜é£é™©").sum()),
                yellow_days=("Risk_final", lambda s: (s == "ğŸŸ¡ é¢„è­¦").sum()),
                activity=("Activity", "sum"),
            )
            .reindex(STATIONS)
            .fillna(0)
            .reset_index()
        )

        a2 = alerts_f.copy()
        if len(a2) > 0:
            a2["created_dt"] = pd.to_datetime(a2["created_at"], errors="coerce")
            a2["closed_dt"] = pd.to_datetime(a2["closed_at"], errors="coerce")
            a2["close_hours"] = (a2["closed_dt"] - a2["created_dt"]).dt.total_seconds() / 3600
            close_eff = a2.groupby("station")["close_hours"].mean().reindex(STATIONS)
            comp["å¹³å‡é—­ç¯æ—¶æ•ˆ(h)"] = comp["station"].map(close_eff).fillna(0).round(1)
        else:
            comp["å¹³å‡é—­ç¯æ—¶æ•ˆ(h)"] = 0

        st.dataframe(comp, use_container_width=True)

    else:
        st.subheader("ğŸ“Œ ç«™ç‚¹å·¥ä½œå°ï¼ˆæœ€æ–°çŠ¶æ€ï¼‰")
        latest = df_f.sort_values(["valve_type", "date"]).groupby("valve_type").tail(1)
        cols = st.columns(3)
        for i, valve in enumerate(["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"]):
            block = latest[latest["valve_type"] == valve]
            if len(block) == 0:
                cols[i].metric(f"{valve} HI", "â€”")
                cols[i].metric("é£é™©", "â€”")
                continue
            row = block.iloc[0]
            cols[i].metric(f"{valve} HI", f"{row['HI_final']:.1f}")
            cols[i].metric("é£é™©", row["Risk_final"])
            cols[i].caption(f"å‹åŠ›å æ•´å®šï¼š{row['ratio'] * 100:.1f}%")

with tab_alert:
    st.subheader("ğŸš¨ å‘Šè­¦ä¸­å¿ƒ")
    if len(alerts_f) == 0:
        st.info("å½“å‰æ—¥æœŸèŒƒå›´æ— å‘Šè­¦ã€‚")
    else:
        show_cols = [
            "id",
            "date",
            "station",
            "valve_type",
            "risk_level",
            "trigger_source",
            "status",
            "owner",
            "action_taken",
            "verification_result",
            "created_at",
            "updated_at",
            "closed_at",
        ]
        st.dataframe(alerts_f[show_cols], use_container_width=True)

    if not IS_LEADER and len(alerts_f) > 0:
        st.markdown("**å¤„ç†å‘Šè­¦**")
        work_alerts = alerts_f.copy()
        selected = st.selectbox(
            "é€‰æ‹©å‘Šè­¦ID",
            work_alerts["id"].astype(str).tolist(),
            index=0,
        )
        row = work_alerts[work_alerts["id"].astype(str) == str(selected)].iloc[0]

        cur_status = row["status"] if row["status"] in STATUS_FLOW else "å¾…ç¡®è®¤"
        cur_i = STATUS_FLOW.index(cur_status)
        next_options = STATUS_FLOW[cur_i : min(cur_i + 2, len(STATUS_FLOW))]

        c1, c2 = st.columns(2)
        with c1:
            st.text_input("å½“å‰çŠ¶æ€", value=cur_status, disabled=True)
        with c2:
            new_status = st.selectbox("ç›®æ ‡çŠ¶æ€", next_options, index=0)

        action_taken = st.text_area("æ•´æ”¹æªæ–½ï¼ˆå…³é—­å‰å¿…å¡«ï¼‰", value=str(row.get("action_taken", "")))
        verification_result = st.text_area("å¤éªŒç»“æœï¼ˆå…³é—­å‰å¿…å¡«ï¼‰", value=str(row.get("verification_result", "")))

        if st.button("æ›´æ–°å‘Šè­¦çŠ¶æ€", use_container_width=True):
            try:
                update_alert_status(
                    alert_id=str(selected),
                    new_status=new_status,
                    operator=st.session_state.user_name,
                    action_taken=action_taken,
                    verification_result=verification_result,
                )
                st.success("å‘Šè­¦çŠ¶æ€å·²æ›´æ–°")
                st.rerun()
            except Exception as ex:
                st.error(f"æ›´æ–°å¤±è´¥ï¼š{ex}")
    elif IS_LEADER:
        st.info("é¢†å¯¼è´¦å·ä¸ºåªè¯»ï¼Œä¸å¯ä¿®æ”¹å‘Šè­¦çŠ¶æ€ã€‚")

with tab_export:
    st.subheader("ğŸ“¥ æŠ¥è¡¨å¯¼å‡º")

    csv_data = df_f.sort_values(["station", "valve_type", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        "ä¸‹è½½ç›‘æµ‹æ•°æ®CSV",
        data=csv_data,
        file_name="psv_data_filtered.csv",
        mime="text/csv",
        use_container_width=True,
    )

    csv_alert = alerts_f.sort_values(["station", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        "ä¸‹è½½å‘Šè­¦æ•°æ®CSV",
        data=csv_alert,
        file_name="psv_alerts_filtered.csv",
        mime="text/csv",
        use_container_width=True,
    )

    avg_hi = df_f["HI_final"].mean()
    red_cnt = int((df_f["Risk_final"] == "ğŸ”´ é«˜é£é™©").sum())
    yellow_cnt = int((df_f["Risk_final"] == "ğŸŸ¡ é¢„è­¦").sum())
    close_rate = 0.0
    if len(alerts_f) > 0:
        close_rate = (alerts_f["status"] == "å·²å…³é—­").mean() * 100

    summary_lines = [
        f"æŠ¥å‘ŠèŒƒå›´ï¼š{start_date} è‡³ {end_date}",
        f"è´¦å·èŒƒå›´ï¼š{STATION_SCOPE}",
        f"å¹³å‡HIï¼š{avg_hi:.1f}",
        f"é«˜é£é™©è®°å½•æ•°ï¼š{red_cnt}",
        f"é¢„è­¦è®°å½•æ•°ï¼š{yellow_cnt}",
        f"å‘Šè­¦é—­ç¯ç‡ï¼š{close_rate:.1f}%",
    ]

    if IS_LEADER:
        comp = (
            df_f.groupby("station")["HI_final"]
            .mean()
            .reindex(STATIONS)
            .fillna(0)
        )
        summary_lines.append(f"ç«™ç‚¹å¯¹æ¯”ï¼šåç›˜å¹³å‡HI={comp.get(STATIONS[0], 0):.1f}ï¼Œç½—æ‰€å¹³å‡HI={comp.get(STATIONS[1], 0):.1f}")

    summary_text = "\n".join(summary_lines)
    st.text_area("ç®¡ç†æ‘˜è¦ï¼ˆå¯ç›´æ¥è´´PPTï¼‰", value=summary_text, height=180)
    st.download_button(
        "ä¸‹è½½ç®¡ç†æ‘˜è¦TXT",
        data=summary_text.encode("utf-8"),
        file_name="management_summary.txt",
        mime="text/plain",
        use_container_width=True,
    )

with tab_history:
    st.subheader("ğŸ“ˆ å†å²åˆ†æ")

    station_opts = sorted(df_f["station"].unique())
    if IS_LEADER:
        station_pick = st.selectbox("ç«™ç‚¹", station_opts, index=0)
    else:
        station_pick = STATION_SCOPE
        st.info(f"å½“å‰ç«™ç‚¹ï¼š{station_pick}")

    sdf = df_f[df_f["station"] == station_pick].copy()
    if len(sdf) == 0:
        st.warning("è¯¥ç«™ç‚¹å½“å‰ç­›é€‰èŒƒå›´å†…æ— æ•°æ®ã€‚")
        st.stop()

    valve_pick = st.selectbox("é˜€é—¨", sorted(sdf["valve_type"].unique()), index=0)
    vdf = sdf[sdf["valve_type"] == valve_pick].sort_values("date").copy()
    vdf["date_dt"] = pd.to_datetime(vdf["date"])

    c1, c2 = st.columns(2)
    with c1:
        fig, ax = plt.subplots()
        ax.plot(vdf["date_dt"], vdf["p_max"], marker="o", label="p_max")
        ax.plot(vdf["date_dt"], vdf["p_now"], marker="o", linestyle="--", label="p_now")
        ax.axhline(SET_P, linestyle="--", label="æ•´å®šå‹åŠ› 1.32MPa")
        ax.set_title(f"{station_pick} - {valve_pick} å‹åŠ›è¶‹åŠ¿")
        ax.set_ylabel("MPa")
        ax.legend()
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
        plt.xticks(rotation=30)
        st.pyplot(fig)

    with c2:
        fig, ax = plt.subplots()
        ax.plot(vdf["date_dt"], vdf["HI_final"], marker="o")
        ax.set_title(f"{station_pick} - {valve_pick} å¥åº·æŒ‡æ•°è¶‹åŠ¿")
        ax.set_ylabel("HI")
        ax.set_ylim(0, 100)
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
        plt.xticks(rotation=30)
        st.pyplot(fig)

    st.markdown("**æœ€è¿‘20æ¡è®°å½•**")
    show_cols = [
        "date",
        "station",
        "valve_type",
        "p_now",
        "p_max",
        "level",
        "temp",
        "psv_act",
        "psv_weeping",
        "HI_final",
        "Risk_final",
        "AI_anomaly",
        "AI_score",
    ]
    st.dataframe(sdf.sort_values("date", ascending=False)[show_cols].head(20), use_container_width=True)
