
import os
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplimport os
import uuid
from datetime import timedelta

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import matplotlib as mpl
from matplotlib import font_manager


# ================== Config ==================
SET_P = 1.32
DATA_FILE = "psv_data.csv"
ALERT_FILE = "psv_alerts.csv"
AUDIT_FILE = "psv_audit_logs.csv"

TABLE_DATA = "psv_data"
TABLE_ALERT = "psv_alerts"
TABLE_AUDIT = "psv_audit_logs"

STATUS_FLOW = ["å¾…ç¡®è®¤", "å·²æ´¾å·¥", "å¤„ç†ä¸­", "å·²éªŒè¯", "å·²å…³é—­"]
STATIONS = ["åç›˜LNGåŠ æ°”ç«™", "ç½—æ‰€LNGåŠ æ°”ç«™"]
DEFAULT_STATION = "åç›˜LNGåŠ æ°”ç«™"


# ================== Supabase init ==================
try:
    from supabase import create_client

    SUPABASE_OK = True
except Exception:
    SUPABASE_OK = False


def _secret_get(key: str, default=""):
    try:
        return st.secrets.get(key, default)
    except Exception:
        return default


SUPABASE_URL = _secret_get("SUPABASE_URL", "https://ynowvxcsvjskwkeauvkz.supabase.co") or os.getenv("SUPABASE_URL", "https://ynowvxcsvjskwkeauvkz.supabase.co")
SUPABASE_KEY = _secret_get("SUPABASE_KEY", "sb_publishable_aezshZPqB78WBtyWtTf8Tg_UVpCEZzd") or os.getenv("SUPABASE_KEY", "sb_publishable_aezshZPqB78WBtyWtTf8Tg_UVpCEZzd")

USE_SUPABASE = True
supabase = None
if USE_SUPABASE and SUPABASE_OK and SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
elif USE_SUPABASE:
    st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ° Supabase é…ç½®ï¼ˆSUPABASE_URL / SUPABASE_KEYï¼‰ï¼Œå°†å›é€€ä¸ºæœ¬åœ°CSVå­˜å‚¨ã€‚")
    USE_SUPABASE = False


# ================== ML ==================
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler

    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False


# ================== Font ==================
def _setup_cjk_font():
    preferred_font_names = [
        "Noto Sans CJK SC",
        "Noto Sans CJK JP",
        "Noto Sans CJK TC",
        "Microsoft YaHei",
        "SimHei",
        "PingFang SC",
        "WenQuanYi Micro Hei",
        "Source Han Sans SC",
    ]

    available = {f.name for f in font_manager.fontManager.ttflist}
    for name in preferred_font_names:
        if name in available:
            mpl.rcParams["font.family"] = "sans-serif"
            mpl.rcParams["font.sans-serif"] = [name, "DejaVu Sans"]
            mpl.rcParams["axes.unicode_minus"] = False
            return

    font_path = "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc"
    if os.path.exists(font_path):
        fp = font_manager.FontProperties(fname=font_path)
        mpl.rcParams["font.family"] = fp.get_name()
        mpl.rcParams["axes.unicode_minus"] = False


_setup_cjk_font()
st.set_page_config(page_title="LNGå®‰å…¨é˜€å¤šç«™ç‚¹ç›‘æµ‹ç³»ç»Ÿ", layout="wide")


# ================== Auth ==================
def load_accounts() -> dict:
    hp = _secret_get("PASSWORD_HUAPAN", "hp123456") or os.getenv("PASSWORD_HUAPAN", "hp123456")
    ls = _secret_get("PASSWORD_LUOSUO", "ls123456") or os.getenv("PASSWORD_LUOSUO", "ls123456")
    leader = _secret_get("PASSWORD_LEADER", "leader123456") or os.getenv("PASSWORD_LEADER", "leader123456")

    return {
        "åç›˜ç«™": {"password": hp, "role": "station", "station_scope": "åç›˜LNGåŠ æ°”ç«™"},
        "ç½—æ‰€ç«™": {"password": ls, "role": "station", "station_scope": "ç½—æ‰€LNGåŠ æ°”ç«™"},
        "é¢†å¯¼": {"password": leader, "role": "leader", "station_scope": "ALL"},
    }


ACCOUNTS = load_accounts()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False
if "user_name" not in st.session_state:
    st.session_state.user_name = ""
if "role" not in st.session_state:
    st.session_state.role = ""
if "station_scope" not in st.session_state:
    st.session_state.station_scope = ""

st.sidebar.title("ğŸ” è®¿é—®æ§åˆ¶")

if not st.session_state.authenticated:
    login_name = st.sidebar.selectbox("è´¦å·", list(ACCOUNTS.keys()), index=0)
    login_pwd = st.sidebar.text_input("å¯†ç ", type="password")
    if st.sidebar.button("ç™»å½•", use_container_width=True):
        if login_pwd == ACCOUNTS[login_name]["password"]:
            st.session_state.authenticated = True
            st.session_state.user_name = login_name
            st.session_state.role = ACCOUNTS[login_name]["role"]
            st.session_state.station_scope = ACCOUNTS[login_name]["station_scope"]
            st.rerun()
        else:
            st.sidebar.error("å¯†ç é”™è¯¯")
    st.warning("è¯·è¾“å…¥è´¦å·å’Œå¯†ç åè¿›å…¥ç³»ç»Ÿã€‚")
    st.stop()
else:
    st.sidebar.success(
        f"å·²ç™»å½•ï¼š{st.session_state.user_name} | è§’è‰²ï¼š{st.session_state.role} | èŒƒå›´ï¼š{st.session_state.station_scope}"
    )
    if st.sidebar.button("é€€å‡ºç™»å½•", use_container_width=True):
        for k in ["authenticated", "user_name", "role", "station_scope"]:
            st.session_state.pop(k, None)
        st.rerun()


ROLE = st.session_state.role
STATION_SCOPE = st.session_state.station_scope
IS_LEADER = ROLE == "leader"


# ================== Data helpers ==================
BASE_DATA_COLS = [
    "date",
    "station",
    "valve_type",
    "p_now",
    "p_max",
    "level",
    "temp",
    "psv_act",
    "psv_weeping",
    "operator_role",
    "operator_name",
    "updated_at",
]

BASE_ALERT_COLS = [
    "id",
    "date",
    "station",
    "valve_type",
    "risk_level",
    "trigger_source",
    "trigger_detail",
    "status",
    "owner",
    "action_taken",
    "verification_result",
    "created_at",
    "updated_at",
    "closed_at",
]

BASE_AUDIT_COLS = ["id", "entity_type", "entity_id", "action", "operator", "payload", "created_at"]


def _ensure_local_files():
    if not os.path.exists(DATA_FILE):
        pd.DataFrame(columns=BASE_DATA_COLS).to_csv(DATA_FILE, index=False, encoding="utf-8-sig")
    if not os.path.exists(ALERT_FILE):
        pd.DataFrame(columns=BASE_ALERT_COLS).to_csv(ALERT_FILE, index=False, encoding="utf-8-sig")
    if not os.path.exists(AUDIT_FILE):
        pd.DataFrame(columns=BASE_AUDIT_COLS).to_csv(AUDIT_FILE, index=False, encoding="utf-8-sig")


_ensure_local_files()


def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or len(df) == 0:
        return df

    rename_map = {
        "Valve_type": "valve_type",
        "P_now": "p_now",
        "P_max": "p_max",
        "Level": "level",
        "Temp": "temp",
        "PSV_act": "psv_act",
        "PSV_weeping": "psv_weeping",
    }
    df = df.rename(columns=rename_map)

    for c in BASE_DATA_COLS:
        if c not in df.columns:
            if c == "station":
                df[c] = DEFAULT_STATION
            elif c in ["operator_role", "operator_name", "updated_at"]:
                df[c] = ""
            else:
                df[c] = np.nan

    return df[BASE_DATA_COLS]


def _normalize_df(df0: pd.DataFrame) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return pd.DataFrame(columns=BASE_DATA_COLS)

    df = _standardize_columns(df0.copy())
    df["station"] = df["station"].fillna(DEFAULT_STATION).replace("", DEFAULT_STATION)
    df["station"] = df["station"].where(df["station"].isin(STATIONS), DEFAULT_STATION)

    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date

    for col in ["p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df["p_now"] = df["p_now"].clip(lower=0, upper=2)
    df["p_max"] = df["p_max"].clip(lower=0, upper=2)
    df["level"] = df["level"].clip(lower=0, upper=100)
    df["temp"] = df["temp"].clip(lower=-50, upper=80)

    m = df["p_max"].notna() & df["p_now"].notna() & (df["p_max"] < df["p_now"])
    if m.any():
        df.loc[m, "p_max"] = df.loc[m, "p_now"]

    df = df.dropna(subset=["date", "station", "valve_type", "p_max"])
    df = (
        df.sort_values(["station", "valve_type", "date", "updated_at"])
        .drop_duplicates(subset=["date", "station", "valve_type"], keep="last")
        .reset_index(drop=True)
    )

    return df


def _scope_filter(df: pd.DataFrame, station_scope: str) -> pd.DataFrame:
    if len(df) == 0:
        return df
    if station_scope == "ALL":
        return df
    return df[df["station"] == station_scope].copy()


def load_data(station_scope: str, role: str) -> pd.DataFrame:
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table(TABLE_DATA).select("*").execute()
        raw = pd.DataFrame(resp.data or [])
    else:
        raw = pd.read_csv(DATA_FILE)

    df = _normalize_df(raw)
    return _scope_filter(df, station_scope)


def _write_local_data(df: pd.DataFrame):
    df.to_csv(DATA_FILE, index=False, encoding="utf-8-sig")


def save_record(record: dict, station_scope: str, role: str) -> None:
    if role == "leader":
        raise PermissionError("é¢†å¯¼è´¦å·ä¸ºåªè¯»ï¼Œä¸å…è®¸å†™å…¥æ•°æ®")

    if station_scope != "ALL" and record.get("station") != station_scope:
        raise PermissionError("åªèƒ½å†™å…¥æœ¬ç«™æ•°æ®")

    record = record.copy()
    record["updated_at"] = pd.Timestamp.now().isoformat()

    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_DATA).upsert(record, on_conflict="date,station,valve_type").execute()
        return

    df_all = _normalize_df(pd.read_csv(DATA_FILE))
    new_row = pd.DataFrame([record])
    merged = pd.concat([df_all, new_row], ignore_index=True)
    merged = _normalize_df(merged)
    _write_local_data(merged)


# ================== Alerts ==================
def _normalize_alert_df(df0: pd.DataFrame) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return pd.DataFrame(columns=BASE_ALERT_COLS)

    df = df0.copy()
    for c in BASE_ALERT_COLS:
        if c not in df.columns:
            df[c] = ""

    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date
    df["station"] = df["station"].fillna(DEFAULT_STATION).replace("", DEFAULT_STATION)
    df["status"] = df["status"].replace("", "å¾…ç¡®è®¤")
    return df[BASE_ALERT_COLS]


def _load_alerts_all() -> pd.DataFrame:
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table(TABLE_ALERT).select("*").execute()
        raw = pd.DataFrame(resp.data or [])
    else:
        raw = pd.read_csv(ALERT_FILE)
    return _normalize_alert_df(raw)


def _save_alerts_local(df: pd.DataFrame):
    df.to_csv(ALERT_FILE, index=False, encoding="utf-8-sig")


def append_audit(entity_type: str, entity_id: str, action: str, operator: str, payload: str):
    log = {
        "id": str(uuid.uuid4()),
        "entity_type": entity_type,
        "entity_id": str(entity_id),
        "action": action,
        "operator": operator,
        "payload": payload,
        "created_at": pd.Timestamp.now().isoformat(),
    }
    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_AUDIT).insert(log).execute()
    else:
        df = pd.read_csv(AUDIT_FILE)
        df = pd.concat([df, pd.DataFrame([log])], ignore_index=True)
        df.to_csv(AUDIT_FILE, index=False, encoding="utf-8-sig")


def _find_alert(alerts: pd.DataFrame, date_value, station: str, valve_type: str):
    m = (
        (alerts["date"] == pd.to_datetime(date_value).date())
        & (alerts["station"] == station)
        & (alerts["valve_type"] == valve_type)
    )
    return alerts[m]


def create_or_update_alert(record: dict):
    now_iso = pd.Timestamp.now().isoformat()
    alerts = _load_alerts_all()

    found = _find_alert(alerts, record["date"], record["station"], record["valve_type"])
    if len(found) > 0:
        idx = found.index[0]
        keep_status = alerts.loc[idx, "status"] or "å¾…ç¡®è®¤"
        alerts.loc[idx, "risk_level"] = record.get("risk_level", alerts.loc[idx, "risk_level"])
        alerts.loc[idx, "trigger_source"] = record.get("trigger_source", alerts.loc[idx, "trigger_source"])
        alerts.loc[idx, "trigger_detail"] = str(record.get("trigger_detail", alerts.loc[idx, "trigger_detail"]))
        alerts.loc[idx, "updated_at"] = now_iso
        alerts.loc[idx, "status"] = keep_status
    else:
        new_alert = {
            "id": str(uuid.uuid4()),
            "date": str(record["date"]),
            "station": record["station"],
            "valve_type": record["valve_type"],
            "risk_level": record.get("risk_level", "ğŸ”´ é«˜é£é™©"),
            "trigger_source": record.get("trigger_source", "rule"),
            "trigger_detail": str(record.get("trigger_detail", "")),
            "status": "å¾…ç¡®è®¤",
            "owner": "",
            "action_taken": "",
            "verification_result": "",
            "created_at": now_iso,
            "updated_at": now_iso,
            "closed_at": "",
        }
        alerts = pd.concat([alerts, pd.DataFrame([new_alert])], ignore_index=True)

    if USE_SUPABASE and supabase is not None:
        rows = alerts.to_dict(orient="records")
        supabase.table(TABLE_ALERT).upsert(rows, on_conflict="id").execute()
    else:
        _save_alerts_local(alerts)


def update_alert_status(alert_id: str, new_status: str, operator: str, action_taken: str = "", verification_result: str = ""):
    alerts = _load_alerts_all()
    hit = alerts[alerts["id"].astype(str) == str(alert_id)]
    if len(hit) == 0:
        raise ValueError("æœªæ‰¾åˆ°å‘Šè­¦")

    idx = hit.index[0]
    cur = alerts.loc[idx, "status"]
    if cur not in STATUS_FLOW:
        cur = "å¾…ç¡®è®¤"
    if new_status not in STATUS_FLOW:
        raise ValueError("éæ³•çŠ¶æ€")

    cur_i = STATUS_FLOW.index(cur)
    new_i = STATUS_FLOW.index(new_status)
    if not (new_i == cur_i or new_i == cur_i + 1):
        raise ValueError("çŠ¶æ€ä»…å…è®¸ä¿æŒä¸å˜æˆ–æ¨è¿›ä¸€æ­¥")

    if new_status == "å·²å…³é—­":
        if not action_taken.strip() or not verification_result.strip():
            raise ValueError("å…³é—­å‘Šè­¦å‰å¿…é¡»å¡«å†™æ•´æ”¹æªæ–½å’Œå¤éªŒç»“æœ")
        alerts.loc[idx, "closed_at"] = pd.Timestamp.now().isoformat()

    if action_taken.strip():
        alerts.loc[idx, "action_taken"] = action_taken.strip()
    if verification_result.strip():
        alerts.loc[idx, "verification_result"] = verification_result.strip()

    alerts.loc[idx, "status"] = new_status
    alerts.loc[idx, "owner"] = operator
    alerts.loc[idx, "updated_at"] = pd.Timestamp.now().isoformat()

    if USE_SUPABASE and supabase is not None:
        supabase.table(TABLE_ALERT).upsert(alerts.to_dict(orient="records"), on_conflict="id").execute()
    else:
        _save_alerts_local(alerts)

    append_audit(
        entity_type="alert",
        entity_id=str(alert_id),
        action=f"status:{cur}->{new_status}",
        operator=operator,
        payload=f"action_taken={action_taken}; verification_result={verification_result}",
    )


def list_alerts(station_scope: str, role: str) -> pd.DataFrame:
    alerts = _load_alerts_all()
    if station_scope != "ALL":
        alerts = alerts[alerts["station"] == station_scope].copy()
    if len(alerts) == 0:
        return alerts
    return alerts.sort_values(["status", "date"], ascending=[True, False]).reset_index(drop=True)


# ================== Scoring ==================
def compute_scores(df0: pd.DataFrame, enable_ai: bool, contamination: float) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return df0

    df = _normalize_df(df0).copy()
    df = df.sort_values(["station", "valve_type", "date"]).reset_index(drop=True)

    df["ratio"] = df["p_max"] / SET_P
    df["slope"] = (
        df.groupby(["station", "valve_type"])["p_max"]
        .apply(lambda s: s.diff().rolling(3).mean())
        .reset_index(level=[0, 1], drop=True)
    )

    hi = np.full(len(df), 100.0)
    hi -= np.where(df["ratio"] >= 1.00, 35, 0)
    hi -= np.where((df["ratio"] >= 0.98) & (df["ratio"] < 1.00), 20, 0)
    hi -= np.where((df["ratio"] >= 0.95) & (df["ratio"] < 0.98), 10, 0)

    hi -= np.where(df["slope"] > 0.01, 10, 0)
    hi -= np.where(df["slope"] > 0.02, 10, 0)

    hi -= df.get("psv_act", 0).fillna(0) * 30
    hi -= df.get("psv_weeping", 0).fillna(0) * 15

    hi -= np.where(
        (df.get("temp", 0).fillna(0) >= 33)
        & (df.get("level", 0).fillna(0) >= 80)
        & (df["ratio"] >= 0.95),
        10,
        0,
    )

    df["HI"] = np.clip(hi, 0, 100)

    def risk(x: float) -> str:
        if x >= 85:
            return "ğŸŸ¢ å®‰å…¨"
        if x >= 70:
            return "ğŸŸ¡ é¢„è­¦"
        return "ğŸ”´ é«˜é£é™©"

    df["Risk"] = df["HI"].apply(risk)
    df["Activity"] = df.get("psv_act", 0).fillna(0) + df.get("psv_weeping", 0).fillna(0)

    df["AI_anomaly"] = False
    df["AI_score"] = np.nan

    if enable_ai and SKLEARN_OK:
        features = ["p_now", "p_max", "level", "temp", "ratio", "slope", "Activity"]
        for _, g in df.groupby(["station", "valve_type"]):
            idx = g.index
            if len(g) < 10:
                continue
            x = g[features].copy().apply(pd.to_numeric, errors="coerce")
            x = x.fillna(x.median(numeric_only=True))

            scaler = StandardScaler()
            xs = scaler.fit_transform(x.values)

            iso = IsolationForest(n_estimators=200, contamination=float(contamination), random_state=42)
            iso.fit(xs)
            pred = iso.predict(xs)
            score = -iso.score_samples(xs)

            df.loc[idx, "AI_anomaly"] = pred == -1
            df.loc[idx, "AI_score"] = score

        df["HI_final"] = np.clip(df["HI"] - df["AI_anomaly"].astype(int) * 10, 0, 100)
    else:
        df["HI_final"] = df["HI"]

    df["Risk_final"] = df["HI_final"].apply(risk)
    return df


def _calc_trigger_source(row: pd.Series):
    rule_hit = str(row.get("Risk_final", "")) == "ğŸ”´ é«˜é£é™©"
    ai_hit = bool(row.get("AI_anomaly", False))

    if rule_hit and ai_hit:
        return "both"
    if rule_hit:
        return "rule"
    if ai_hit:
        return "ai"
    return ""


def sync_alerts_from_scores(df_scored: pd.DataFrame):
    if df_scored is None or len(df_scored) == 0:
        return

    for _, row in df_scored.iterrows():
        source = _calc_trigger_source(row)
        if not source:
            continue
        create_or_update_alert(
            {
                "date": row["date"],
                "station": row["station"],
                "valve_type": row["valve_type"],
                "risk_level": row.get("Risk_final", "ğŸ”´ é«˜é£é™©"),
                "trigger_source": source,
                "trigger_detail": {
                    "HI_final": float(row.get("HI_final", np.nan)),
                    "ratio": float(row.get("ratio", np.nan)),
                    "AI_anomaly": bool(row.get("AI_anomaly", False)),
                },
            }
        )


# ================== UI ==================
st.title("LNGå®‰å…¨é˜€å¤šç«™ç‚¹AIå¥åº·ç›‘æµ‹ä¸å‘Šè­¦é—­ç¯ç³»ç»Ÿ")
st.caption("ä¸€æœŸï¼šåç›˜ç«™/ç½—æ‰€ç«™/é¢†å¯¼ä¸‰è´¦å·ï¼ŒæŒ‰ç«™ç‚¹æ•°æ®éš”ç¦»ï¼Œé¢†å¯¼åªè¯»ã€‚")

st.sidebar.divider()
st.sidebar.header("ğŸ§  AI å¼‚å¸¸æ£€æµ‹")
enable_ai = st.sidebar.checkbox("å¯ç”¨ AI å¼‚å¸¸æ£€æµ‹", value=True)
contamination = st.sidebar.slider("å¼‚å¸¸æ¯”ä¾‹ï¼ˆè¶Šå¤§è¶Šæ•æ„Ÿï¼‰", min_value=0.02, max_value=0.20, value=0.08, step=0.01)
if enable_ai and not SKLEARN_OK:
    st.sidebar.warning("å½“å‰ç¯å¢ƒç¼ºå°‘ scikit-learnï¼ŒAIå¼‚å¸¸æ£€æµ‹ä¸å¯ç”¨ã€‚")
    enable_ai = False

if not IS_LEADER:
    st.sidebar.divider()
    st.sidebar.header("ğŸ“ æœ¬ç«™æ•°æ®å½•å…¥")
    st.sidebar.info(f"å½“å‰ç«™ç‚¹ï¼š{STATION_SCOPE}")

    valve_type = st.sidebar.selectbox("é€‰æ‹©å®‰å…¨é˜€ç±»å‹", ["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"])
    date = st.sidebar.date_input("æ—¥æœŸ")
    p_now = st.sidebar.number_input("å½“å‰å‹åŠ› p_now (MPa)", 0.0, 2.0, 1.20, 0.01)
    p_max = st.sidebar.number_input(
        "å½“æ—¥æœ€é«˜å‹åŠ› p_max (MPa)",
        0.0,
        2.0,
        1.20,
        0.01,
        help="å»ºè®®ï¼šp_max â‰¥ p_nowï¼›è‹¥è¾“å…¥å°äº p_nowï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰ p_now ä¿®æ­£ã€‚",
    )
    level = st.sidebar.number_input("æ¶²ä½ level (%)", 0, 100, 60)
    temp = st.sidebar.number_input("ç¯å¢ƒæ¸©åº¦ temp (â„ƒ)", -30, 60, 25)
    psv_act = st.sidebar.selectbox("æ˜¯å¦åŠ¨ä½œ", ["å¦", "æ˜¯"])
    psv_weeping = st.sidebar.selectbox("æ˜¯å¦å¾®æ”¾æ•£/å˜¶å˜¶å£°", ["å¦", "æ˜¯"])

    if st.sidebar.button("ä¿å­˜å¹¶è®¡ç®—", use_container_width=True):
        p_now_f = float(p_now)
        p_max_f = float(p_max)
        if p_max_f < p_now_f:
            st.sidebar.warning(f"å·²è‡ªåŠ¨ä¿®æ­£ï¼šp_max({p_max_f:.2f}) < p_now({p_now_f:.2f})ï¼Œå°† p_max è®¾ä¸º {p_now_f:.2f}")
            p_max_f = p_now_f

        try:
            save_record(
                {
                    "date": str(date),
                    "station": STATION_SCOPE,
                    "valve_type": valve_type,
                    "p_now": p_now_f,
                    "p_max": p_max_f,
                    "level": int(level),
                    "temp": int(temp),
                    "psv_act": 1 if psv_act == "æ˜¯" else 0,
                    "psv_weeping": 1 if psv_weeping == "æ˜¯" else 0,
                    "operator_role": ROLE,
                    "operator_name": st.session_state.user_name,
                },
                station_scope=STATION_SCOPE,
                role=ROLE,
            )
            st.sidebar.success("âœ… æ•°æ®å·²ä¿å­˜")
            st.rerun()
        except Exception as ex:
            st.sidebar.error(f"ä¿å­˜å¤±è´¥ï¼š{ex}")
else:
    st.sidebar.info("é¢†å¯¼è´¦å·ä¸ºåªè¯»æ¨¡å¼ï¼Œä¸å¯å½•å…¥æˆ–ä¿®æ”¹åŸå§‹æ•°æ®ã€‚")


# Load + score + auto alerts
df_raw = load_data(station_scope=STATION_SCOPE, role=ROLE)
df = compute_scores(df_raw, enable_ai=enable_ai, contamination=contamination)
sync_alerts_from_scores(df)
alerts = list_alerts(station_scope=STATION_SCOPE, role=ROLE)

if len(df) == 0:
    st.info("å½“å‰æƒé™èŒƒå›´å†…è¿˜æ²¡æœ‰æ•°æ®ã€‚")
    st.stop()

# Common date filter
min_d, max_d = df["date"].min(), df["date"].max()
c1, c2, c3 = st.columns([1, 1, 2])
with c1:
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=min_d, min_value=min_d, max_value=max_d, key="start")
with c2:
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=max_d, min_value=min_d, max_value=max_d, key="end")
with c3:
    st.caption("å»ºè®®ï¼šæ±‡æŠ¥åœºæ™¯ä¼˜å…ˆé€‰æ‹©æœ€è¿‘30å¤©ã€‚")

df_f = df[(df["date"] >= start_date) & (df["date"] <= end_date)].copy()
alerts_f = alerts[
    (pd.to_datetime(alerts["date"], errors="coerce").dt.date >= start_date)
    & (pd.to_datetime(alerts["date"], errors="coerce").dt.date <= end_date)
].copy()

if IS_LEADER:
    tab_dashboard, tab_alert, tab_export, tab_history = st.tabs(["é¢†å¯¼é©¾é©¶èˆ±", "å‘Šè­¦ä¸­å¿ƒ", "æŠ¥è¡¨å¯¼å‡º", "å†å²åˆ†æ"])
else:
    tab_dashboard, tab_alert, tab_export, tab_history = st.tabs(["ç«™ç‚¹å·¥ä½œå°", "å‘Šè­¦ä¸­å¿ƒ", "æŠ¥è¡¨å¯¼å‡º", "å†å²åˆ†æ"])

with tab_dashboard:
    if IS_LEADER:
        st.subheader("ğŸ“Š é¢†å¯¼é©¾é©¶èˆ±")
        today = df_f["date"].max()
        week_start = today - timedelta(days=6)
        prev_week_start = today - timedelta(days=13)
        prev_week_end = today - timedelta(days=7)

        today_high_risk = int(((df_f["date"] == today) & (df_f["Risk_final"] == "ğŸ”´ é«˜é£é™©")).sum())
        alerts_week = alerts_f[pd.to_datetime(alerts_f["date"]).dt.date >= week_start]
        new_alerts_week = len(alerts_week)
        closed_week = int((alerts_week["status"] == "å·²å…³é—­").sum()) if len(alerts_week) else 0
        close_rate = (closed_week / new_alerts_week * 100) if new_alerts_week else 0

        cur_week_hi = df_f[(df_f["date"] >= week_start)]["HI_final"].mean()
        prev_week_hi = df_f[(df_f["date"] >= prev_week_start) & (df_f["date"] <= prev_week_end)]["HI_final"].mean()
        hi_delta = 0 if np.isnan(cur_week_hi) or np.isnan(prev_week_hi) else cur_week_hi - prev_week_hi

        k1, k2, k3, k4 = st.columns(4)
        k1.metric("å½“æ—¥é«˜é£é™©é˜€é—¨æ•°", today_high_risk)
        k2.metric("æœ¬å‘¨æ–°å¢å‘Šè­¦æ•°", new_alerts_week)
        k3.metric("æœ¬å‘¨é—­ç¯ç‡", f"{close_rate:.1f}%")
        k4.metric("å¹³å‡HIè¾ƒä¸Šå‘¨", f"{hi_delta:+.1f}")

        st.markdown("**åç›˜ vs ç½—æ‰€ å¯¹æ¯”**")
        comp = (
            df_f.groupby("station")
            .agg(
                avg_HI=("HI_final", "mean"),
                red_days=("Risk_final", lambda s: (s == "ğŸ”´ é«˜é£é™©").sum()),
                yellow_days=("Risk_final", lambda s: (s == "ğŸŸ¡ é¢„è­¦").sum()),
                activity=("Activity", "sum"),
            )
            .reindex(STATIONS)
            .fillna(0)
            .reset_index()
        )

        a2 = alerts_f.copy()
        if len(a2) > 0:
            a2["created_dt"] = pd.to_datetime(a2["created_at"], errors="coerce")
            a2["closed_dt"] = pd.to_datetime(a2["closed_at"], errors="coerce")
            a2["close_hours"] = (a2["closed_dt"] - a2["created_dt"]).dt.total_seconds() / 3600
            close_eff = a2.groupby("station")["close_hours"].mean().reindex(STATIONS)
            comp["å¹³å‡é—­ç¯æ—¶æ•ˆ(h)"] = comp["station"].map(close_eff).fillna(0).round(1)
        else:
            comp["å¹³å‡é—­ç¯æ—¶æ•ˆ(h)"] = 0

        st.dataframe(comp, use_container_width=True)

    else:
        st.subheader("ğŸ“Œ ç«™ç‚¹å·¥ä½œå°ï¼ˆæœ€æ–°çŠ¶æ€ï¼‰")
        latest = df_f.sort_values(["valve_type", "date"]).groupby("valve_type").tail(1)
        cols = st.columns(3)
        for i, valve in enumerate(["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"]):
            block = latest[latest["valve_type"] == valve]
            if len(block) == 0:
                cols[i].metric(f"{valve} HI", "â€”")
                cols[i].metric("é£é™©", "â€”")
                continue
            row = block.iloc[0]
            cols[i].metric(f"{valve} HI", f"{row['HI_final']:.1f}")
            cols[i].metric("é£é™©", row["Risk_final"])
            cols[i].caption(f"å‹åŠ›å æ•´å®šï¼š{row['ratio'] * 100:.1f}%")

with tab_alert:
    st.subheader("ğŸš¨ å‘Šè­¦ä¸­å¿ƒ")
    if len(alerts_f) == 0:
        st.info("å½“å‰æ—¥æœŸèŒƒå›´æ— å‘Šè­¦ã€‚")
    else:
        show_cols = [
            "id",
            "date",
            "station",
            "valve_type",
            "risk_level",
            "trigger_source",
            "status",
            "owner",
            "action_taken",
            "verification_result",
            "created_at",
            "updated_at",
            "closed_at",
        ]
        st.dataframe(alerts_f[show_cols], use_container_width=True)

    if not IS_LEADER and len(alerts_f) > 0:
        st.markdown("**å¤„ç†å‘Šè­¦**")
        work_alerts = alerts_f.copy()
        selected = st.selectbox(
            "é€‰æ‹©å‘Šè­¦ID",
            work_alerts["id"].astype(str).tolist(),
            index=0,
        )
        row = work_alerts[work_alerts["id"].astype(str) == str(selected)].iloc[0]

        cur_status = row["status"] if row["status"] in STATUS_FLOW else "å¾…ç¡®è®¤"
        cur_i = STATUS_FLOW.index(cur_status)
        next_options = STATUS_FLOW[cur_i : min(cur_i + 2, len(STATUS_FLOW))]

        c1, c2 = st.columns(2)
        with c1:
            st.text_input("å½“å‰çŠ¶æ€", value=cur_status, disabled=True)
        with c2:
            new_status = st.selectbox("ç›®æ ‡çŠ¶æ€", next_options, index=0)

        action_taken = st.text_area("æ•´æ”¹æªæ–½ï¼ˆå…³é—­å‰å¿…å¡«ï¼‰", value=str(row.get("action_taken", "")))
        verification_result = st.text_area("å¤éªŒç»“æœï¼ˆå…³é—­å‰å¿…å¡«ï¼‰", value=str(row.get("verification_result", "")))

        if st.button("æ›´æ–°å‘Šè­¦çŠ¶æ€", use_container_width=True):
            try:
                update_alert_status(
                    alert_id=str(selected),
                    new_status=new_status,
                    operator=st.session_state.user_name,
                    action_taken=action_taken,
                    verification_result=verification_result,
                )
                st.success("å‘Šè­¦çŠ¶æ€å·²æ›´æ–°")
                st.rerun()
            except Exception as ex:
                st.error(f"æ›´æ–°å¤±è´¥ï¼š{ex}")
    elif IS_LEADER:
        st.info("é¢†å¯¼è´¦å·ä¸ºåªè¯»ï¼Œä¸å¯ä¿®æ”¹å‘Šè­¦çŠ¶æ€ã€‚")

with tab_export:
    st.subheader("ğŸ“¥ æŠ¥è¡¨å¯¼å‡º")

    csv_data = df_f.sort_values(["station", "valve_type", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        "ä¸‹è½½ç›‘æµ‹æ•°æ®CSV",
        data=csv_data,
        file_name="psv_data_filtered.csv",
        mime="text/csv",
        use_container_width=True,
    )

    csv_alert = alerts_f.sort_values(["station", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        "ä¸‹è½½å‘Šè­¦æ•°æ®CSV",
        data=csv_alert,
        file_name="psv_alerts_filtered.csv",
        mime="text/csv",
        use_container_width=True,
    )

    avg_hi = df_f["HI_final"].mean()
    red_cnt = int((df_f["Risk_final"] == "ğŸ”´ é«˜é£é™©").sum())
    yellow_cnt = int((df_f["Risk_final"] == "ğŸŸ¡ é¢„è­¦").sum())
    close_rate = 0.0
    if len(alerts_f) > 0:
        close_rate = (alerts_f["status"] == "å·²å…³é—­").mean() * 100

    summary_lines = [
        f"æŠ¥å‘ŠèŒƒå›´ï¼š{start_date} è‡³ {end_date}",
        f"è´¦å·èŒƒå›´ï¼š{STATION_SCOPE}",
        f"å¹³å‡HIï¼š{avg_hi:.1f}",
        f"é«˜é£é™©è®°å½•æ•°ï¼š{red_cnt}",
        f"é¢„è­¦è®°å½•æ•°ï¼š{yellow_cnt}",
        f"å‘Šè­¦é—­ç¯ç‡ï¼š{close_rate:.1f}%",
    ]

    if IS_LEADER:
        comp = (
            df_f.groupby("station")["HI_final"]
            .mean()
            .reindex(STATIONS)
            .fillna(0)
        )
        summary_lines.append(f"ç«™ç‚¹å¯¹æ¯”ï¼šåç›˜å¹³å‡HI={comp.get(STATIONS[0], 0):.1f}ï¼Œç½—æ‰€å¹³å‡HI={comp.get(STATIONS[1], 0):.1f}")

    summary_text = "\n".join(summary_lines)
    st.text_area("ç®¡ç†æ‘˜è¦ï¼ˆå¯ç›´æ¥è´´PPTï¼‰", value=summary_text, height=180)
    st.download_button(
        "ä¸‹è½½ç®¡ç†æ‘˜è¦TXT",
        data=summary_text.encode("utf-8"),
        file_name="management_summary.txt",
        mime="text/plain",
        use_container_width=True,
    )

with tab_history:
    st.subheader("ğŸ“ˆ å†å²åˆ†æ")

    station_opts = sorted(df_f["station"].unique())
    if IS_LEADER:
        station_pick = st.selectbox("ç«™ç‚¹", station_opts, index=0)
    else:
        station_pick = STATION_SCOPE
        st.info(f"å½“å‰ç«™ç‚¹ï¼š{station_pick}")

    sdf = df_f[df_f["station"] == station_pick].copy()
    if len(sdf) == 0:
        st.warning("è¯¥ç«™ç‚¹å½“å‰ç­›é€‰èŒƒå›´å†…æ— æ•°æ®ã€‚")
        st.stop()

    valve_pick = st.selectbox("é˜€é—¨", sorted(sdf["valve_type"].unique()), index=0)
    vdf = sdf[sdf["valve_type"] == valve_pick].sort_values("date").copy()
    vdf["date_dt"] = pd.to_datetime(vdf["date"])

    c1, c2 = st.columns(2)
    with c1:
        fig, ax = plt.subplots()
        ax.plot(vdf["date_dt"], vdf["p_max"], marker="o", label="p_max")
        ax.plot(vdf["date_dt"], vdf["p_now"], marker="o", linestyle="--", label="p_now")
        ax.axhline(SET_P, linestyle="--", label="æ•´å®šå‹åŠ› 1.32MPa")
        ax.set_title(f"{station_pick} - {valve_pick} å‹åŠ›è¶‹åŠ¿")
        ax.set_ylabel("MPa")
        ax.legend()
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
        plt.xticks(rotation=30)
        st.pyplot(fig)

    with c2:
        fig, ax = plt.subplots()
        ax.plot(vdf["date_dt"], vdf["HI_final"], marker="o")
        ax.set_title(f"{station_pick} - {valve_pick} å¥åº·æŒ‡æ•°è¶‹åŠ¿")
        ax.set_ylabel("HI")
        ax.set_ylim(0, 100)
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
        plt.xticks(rotation=30)
        st.pyplot(fig)

    st.markdown("**æœ€è¿‘20æ¡è®°å½•**")
    show_cols = [
        "date",
        "station",
        "valve_type",
        "p_now",
        "p_max",
        "level",
        "temp",
        "psv_act",
        "psv_weeping",
        "HI_final",
        "Risk_final",
        "AI_anomaly",
        "AI_score",
    ]
    st.dataframe(sdf.sort_values("date", ascending=False)[show_cols].head(20), use_container_width=True)
ot as plt
import matplotlib.dates as mdates

import matplotlib as mpl
from matplotlib import font_manager

# ================== æ•°æ®å­˜å‚¨ï¼šSupabaseï¼ˆå¤šäººå…±äº«ï¼‰ ==================
# è¯´æ˜ï¼šä¸æ”¹å˜ä½ é¡¹ç›®ä»»ä½•ä¸šåŠ¡é€»è¾‘ï¼ŒåªæŠŠâ€œæœ¬åœ°CSVâ€æ›¿æ¢ä¸ºâ€œSupabaseäº‘æ•°æ®åº“â€ã€‚
# å»ºè®®æŠŠå¯†é’¥æ”¾åœ¨ Streamlit Secrets æˆ–ç¯å¢ƒå˜é‡é‡Œï¼Œé¿å…å†™è¿›ä»£ç ã€‚

try:
    from supabase import create_client
    SUPABASE_OK = True
except Exception:
    SUPABASE_OK = False

# ä¼˜å…ˆä» st.secrets è¯»å–ï¼Œå…¶æ¬¡ä»ç¯å¢ƒå˜é‡è¯»å–
SUPABASE_URL = None
SUPABASE_KEY = None
try:
    SUPABASE_URL = st.secrets.get("SUPABASE_URL", None)
    SUPABASE_KEY = st.secrets.get("SUPABASE_KEY", None)
except Exception:
    pass

SUPABASE_URL = SUPABASE_URL or os.getenv("SUPABASE_URL", "")
SUPABASE_KEY = SUPABASE_KEY or os.getenv("SUPABASE_KEY", "")

# ä½ å¯ä»¥æŠŠ USE_SUPABASE è®¾ä¸º True å¼ºåˆ¶ä½¿ç”¨äº‘ç«¯ï¼›å¦‚æœå¯†é’¥ç¼ºå¤±ä¼šè‡ªåŠ¨å›é€€æœ¬åœ°CSVï¼ˆä¾¿äºä½ æœ¬åœ°è°ƒè¯•ï¼‰
USE_SUPABASE = True
supabase = None
if USE_SUPABASE and SUPABASE_OK and SUPABASE_URL and SUPABASE_KEY:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
elif USE_SUPABASE:
    # å¯†é’¥ç¼ºå¤±æ—¶æç¤ºä¸€æ¬¡ï¼Œä½†ä»å…è®¸å›é€€æœ¬åœ°CSVä»¥å…ç¨‹åºç›´æ¥æŒ‚æ‰
    st.sidebar.warning("âš ï¸ æœªæ£€æµ‹åˆ° Supabase é…ç½®ï¼ˆSUPABASE_URL / SUPABASE_KEYï¼‰ï¼Œå°†å›é€€ä¸ºæœ¬åœ°CSVå­˜å‚¨ã€‚")
    USE_SUPABASE = False

# =====================================================================


# ================== æœºå™¨å­¦ä¹ ï¼šIsolation Forestï¼ˆæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼‰ ==================
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False


# ================== Matplotlib ä¸­æ–‡å­—ä½“ä¿®å¤ï¼ˆé¿å…å›¾è¡¨æ ‡é¢˜/æ ‡ç­¾ä¹±ç ï¼‰ ==================
def _setup_cjk_font():
    # ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿä¸­å¸¸è§çš„ä¸­æ–‡å­—ä½“ï¼›åœ¨å¤šæ•° Linux ç¯å¢ƒä¸‹ NotoSansCJK å¯ç”¨
    preferred_font_names = [
        "Noto Sans CJK SC",
        "Noto Sans CJK JP",
        "Noto Sans CJK TC",
        "Microsoft YaHei",
        "SimHei",
        "PingFang SC",
        "WenQuanYi Micro Hei",
        "Source Han Sans SC",
    ]

    available = {f.name for f in font_manager.fontManager.ttflist}
    for name in preferred_font_names:
        if name in available:
            mpl.rcParams["font.family"] = "sans-serif"
            mpl.rcParams["font.sans-serif"] = [name, "DejaVu Sans"]
            mpl.rcParams["axes.unicode_minus"] = False
            return

    # å…œåº•ï¼šç›´æ¥æŒ‡å®š NotoSansCJK å­—ä½“æ–‡ä»¶ï¼ˆå®¹å™¨é‡Œé€šå¸¸æœ‰ï¼‰
    font_path = "/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc"
    if os.path.exists(font_path):
        fp = font_manager.FontProperties(fname=font_path)
        mpl.rcParams["font.family"] = fp.get_name()
        mpl.rcParams["axes.unicode_minus"] = False

_setup_cjk_font()

# ================== é…ç½®åŒº ==================
SET_P = 1.32  # å®‰å…¨é˜€æ•´å®šå‹åŠ›ï¼ˆMPaï¼‰
DATA_FILE = "psv_data.csv"
APP_PASSWORD = "adsf0608"  # ç®€å•å£ä»¤ï¼ˆå¯åç»­æ¢æˆè´¦å·ä½“ç³»ï¼‰

st.set_page_config(page_title="LNGå®‰å…¨é˜€å¥åº·ç›‘æµ‹ç³»ç»Ÿ", layout="wide")

# ================== ç™»å½• ==================
st.sidebar.title("ğŸ” è®¿é—®æ§åˆ¶")
user_password = st.sidebar.text_input("è¯·è¾“å…¥å¯†ç ", type="password")
if user_password != APP_PASSWORD:
    st.warning("è¯·è¾“å…¥æ­£ç¡®å¯†ç åè¿›å…¥ç³»ç»Ÿ å½“å‰ç‰ˆæœ¬v0.2 å¼€å‘ï¼šYXYã€‚")
    st.stop()


# ================== AI è®¾ç½®ï¼ˆæ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼‰ ==================
st.sidebar.divider()
st.sidebar.header("ğŸ§  AI å¼‚å¸¸æ£€æµ‹ï¼ˆIsolation Forestï¼‰")
enable_ai = st.sidebar.checkbox("å¯ç”¨ AI å¼‚å¸¸æ£€æµ‹", value=True)
contamination = st.sidebar.slider("å¼‚å¸¸æ¯”ä¾‹ï¼ˆè¶Šå¤§è¶Šæ•æ„Ÿï¼‰", min_value=0.02, max_value=0.20, value=0.08, step=0.01)

if enable_ai and not SKLEARN_OK:
    st.sidebar.warning("å½“å‰ç¯å¢ƒç¼ºå°‘ scikit-learnï¼ŒAI å¼‚å¸¸æ£€æµ‹ä¸å¯ç”¨ã€‚å¯æ‰§è¡Œï¼špip install scikit-learn")
    enable_ai = False

# ================== æ ‡é¢˜ ==================
st.title("ç‰æºªé”€å”®åŠ æ°”ç«™ LNG å®‰å…¨é˜€ AI å¥åº·ç›‘æµ‹ä¸å¼‚å¸¸è¯†åˆ«ç³»ç»Ÿ")
st.caption("åŸºäºæ¯æ—¥äººå·¥ä¸ŠæŠ¥æ•°æ®çš„é£é™©é¢„è­¦ã€è¶‹åŠ¿åˆ†æä¸æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆIsolation Forestï¼‰ï¼ˆæ•´å®šå‹åŠ›ï¼š1.32 MPaï¼‰")

# ================== åˆå§‹åŒ–æ•°æ®æ–‡ä»¶/äº‘ç«¯è¡¨ ==================
# ä½ åŸå…ˆç”¨ CSV å­˜å‚¨ï¼›è¿™é‡Œä¿æŒ CSV é€»è¾‘ä¸åˆ ï¼Œåªæ˜¯åœ¨ USE_SUPABASE=True ä¸”é…ç½®é½å…¨æ—¶æ”¹ç”¨ Supabase è¡¨ï¼špsv_data
if not USE_SUPABASE:
    if not os.path.exists(DATA_FILE):
        df_init = pd.DataFrame(
            columns=["date", "valve_type", "p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]
        )
        df_init.to_csv(DATA_FILE, index=False, encoding="utf-8-sig")

def load_data() -> pd.DataFrame:
    # ---- äº‘ç«¯ï¼šSupabase ----
    if USE_SUPABASE and supabase is not None:
        resp = supabase.table("psv_data").select("*").execute()
        data = resp.data or []
        df0 = pd.DataFrame(data)
        if len(df0) == 0:
            return df0
        # Supabase è¿”å›çš„ date å¯èƒ½æ˜¯å­—ç¬¦ä¸²
        df0["date"] = pd.to_datetime(df0["date"]).dt.date
        # å…œåº•ï¼šé˜²æ­¢å­—ç¬¦ä¸²/ç©ºå€¼
        for col in ["p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]:
            if col in df0.columns:
                df0[col] = pd.to_numeric(df0[col], errors="coerce")
        df0 = df0.dropna(subset=["date", "valve_type", "p_max"])
        return _normalize_df(df0)

    # ---- æœ¬åœ°ï¼šCSVï¼ˆå›é€€ï¼‰----
    df0 = pd.read_csv(DATA_FILE)
    if len(df0) == 0:
        return df0
    df0["date"] = pd.to_datetime(df0["date"]).dt.date
    # å…œåº•ï¼šé˜²æ­¢å­—ç¬¦ä¸²/ç©ºå€¼
    for col in ["p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]:
        if col in df0.columns:
            df0[col] = pd.to_numeric(df0[col], errors="coerce")
    df0 = df0.dropna(subset=["date", "valve_type", "p_max"])
    return _normalize_df(df0)




def _normalize_df(df0: pd.DataFrame) -> pd.DataFrame:
    """ç»Ÿä¸€åšæ•°æ®æ¸…æ´—/çº¦æŸï¼Œé¿å…å½•å…¥æˆ–å­˜å‚¨å¯¼è‡´çš„å›¾è¡¨å¼‚å¸¸ã€‚"""
    if df0 is None or len(df0) == 0:
        return df0

    df = df0.copy()

    # ç±»å‹å…œåº•
    df["date"] = pd.to_datetime(df["date"]).dt.date
    for col in ["p_now", "p_max", "level", "temp", "psv_act", "psv_weeping"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # åˆç†èŒƒå›´ï¼ˆé˜²æ­¢è¯¯å½•ï¼‰
    if "p_now" in df.columns:
        df["p_now"] = df["p_now"].clip(lower=0, upper=2)
    if "p_max" in df.columns:
        df["p_max"] = df["p_max"].clip(lower=0, upper=2)
    if "level" in df.columns:
        df["level"] = df["level"].clip(lower=0, upper=100)
    if "temp" in df.columns:
        df["temp"] = df["temp"].clip(lower=-50, upper=80)

    # ç‰©ç†çº¦æŸï¼šå½“æ—¥æœ€é«˜å‹åŠ› >= å½“å‰å‹åŠ›ï¼ˆè‹¥è¿åï¼ŒæŒ‰ p_now ä¿®æ­£ p_maxï¼‰
    if "p_now" in df.columns and "p_max" in df.columns:
        m = df["p_max"].notna() & df["p_now"].notna() & (df["p_max"] < df["p_now"])
        if m.any():
            df.loc[m, "p_max"] = df.loc[m, "p_now"]

    # åŒä¸€é˜€é—¨åŒä¸€å¤©é‡å¤å½•å…¥ï¼šä¿ç•™æœ€åä¸€æ¡ï¼ˆé¿å…å›¾è¡¨â€œçœ‹èµ·æ¥ä¸å¯¹â€ï¼‰
    if set(["date", "valve_type"]).issubset(df.columns):
        df = df.sort_values(["valve_type", "date"]).drop_duplicates(
            subset=["date", "valve_type"], keep="last"
        )

    df = df.dropna(subset=["date", "valve_type", "p_max"])
    return df
def compute_scores(df0: pd.DataFrame, enable_ai: bool, contamination: float) -> pd.DataFrame:
    if df0 is None or len(df0) == 0:
        return df0

    df0 = _normalize_df(df0)
    if df0 is None or len(df0) == 0:
        return df0

    df = df0.copy()
    df = df.sort_values(["valve_type", "date"]).reset_index(drop=True)

    # ratioï¼šå½“æ—¥æœ€é«˜å‹åŠ›æ¥è¿‘æ•´å®šå‹åŠ›çš„ç¨‹åº¦
    df["ratio"] = df["p_max"] / SET_P

    # slopeï¼š3æ—¥å‹åŠ›å˜åŒ–è¶‹åŠ¿ï¼ˆæŒ‰é˜€é—¨åˆ†ç»„ï¼‰
    df["slope"] = (
        df.groupby("valve_type")["p_max"]
        .apply(lambda s: s.diff().rolling(3).mean())
        .reset_index(level=0, drop=True)
    )

    HI = np.full(len(df), 100.0)

    # A) æ¥è¿‘æ•´å®šå‹åŠ›æ‰£åˆ†ï¼ˆåˆ†æ®µï¼‰
    HI -= np.where(df["ratio"] >= 1.00, 35, 0)
    HI -= np.where((df["ratio"] >= 0.98) & (df["ratio"] < 1.00), 20, 0)
    HI -= np.where((df["ratio"] >= 0.95) & (df["ratio"] < 0.98), 10, 0)

    # B) è¿ç»­ä¸Šå‡è¶‹åŠ¿æ‰£åˆ†ï¼ˆ3æ—¥å‡å€¼ï¼‰
    HI -= np.where(df["slope"] > 0.01, 10, 0)   # 3å¤©å¹³å‡æ¯å¤© +0.01MPa
    HI -= np.where(df["slope"] > 0.02, 10, 0)   # æ›´é™¡å†æ‰£ä¸€æ¬¡

    # C) åŠ¨ä½œ/å¾®æ”¾æ•£æ‰£åˆ†ï¼ˆç»´æŠ¤è§¦å‘ä¿¡å·ï¼‰
    HI -= df.get("psv_act", 0).fillna(0) * 30
    HI -= df.get("psv_weeping", 0).fillna(0) * 15

    # D) é«˜æ¸© + é«˜æ¶²ä½ + é«˜å‹åŠ›ï¼ˆé£é™©å åŠ å› å­ï¼‰
    HI -= np.where(
        (df.get("temp", 0).fillna(0) >= 33)
        & (df.get("level", 0).fillna(0) >= 80)
        & (df["ratio"] >= 0.95),
        10,
        0,
    )

    df["HI"] = np.clip(HI, 0, 100)

    def risk(x: float) -> str:
        if x >= 85:
            return "ğŸŸ¢ å®‰å…¨"
        if x >= 70:
            return "ğŸŸ¡ é¢„è­¦"
        return "ğŸ”´ é«˜é£é™©"

    df["Risk"] = df["HI"].apply(risk)
    df["Activity"] = df.get("psv_act", 0).fillna(0) + df.get("psv_weeping", 0).fillna(0)



    # ================== AI å¼‚å¸¸æ£€æµ‹ï¼ˆIsolation Forestï¼‰ ==================
    # è¯´æ˜ï¼šæ— ç›‘ç£ç®—æ³•ï¼Œä¸éœ€è¦æ•…éšœæ ‡ç­¾ï¼›ç”¨äºå‘ç°â€œæ¨¡å¼å¼‚å¸¸â€çš„è¿è¡Œæ—¥ï¼Œè¡¥è¶³è§„åˆ™é˜ˆå€¼çš„ç›²åŒºã€‚
    df["AI_anomaly"] = False
    df["AI_score"] = np.nan

    if enable_ai and SKLEARN_OK:
        features = ["p_now", "p_max", "level", "temp", "ratio", "slope", "Activity"]
        for valve, g in df.groupby("valve_type"):
            idx = g.index
            # æ•°æ®å¤ªå°‘æ—¶ä¸åšAIï¼ˆé¿å…è¯¯æŠ¥ï¼‰
            if len(g) < 10:
                continue

            X = g[features].copy()
            # ç¼ºå¤±å€¼ç”¨è¯¥é˜€é—¨çš„ä¸­ä½æ•°å¡«å……
            X = X.apply(pd.to_numeric, errors="coerce")
            X = X.fillna(X.median(numeric_only=True))

            scaler = StandardScaler()
            Xs = scaler.fit_transform(X.values)

            iso = IsolationForest(
                n_estimators=200,
                contamination=float(contamination),
                random_state=42,
            )
            iso.fit(Xs)

            pred = iso.predict(Xs)  # -1=å¼‚å¸¸, 1=æ­£å¸¸
            score = -iso.score_samples(Xs)  # å€¼è¶Šå¤§è¶Šâ€œå¼‚å¸¸â€

            df.loc[idx, "AI_anomaly"] = (pred == -1)
            df.loc[idx, "AI_score"] = score

        # å°†AIå¼‚å¸¸ä½œä¸ºâ€œé¢å¤–é£é™©å› å­â€èåˆåˆ°å¥åº·æŒ‡æ•°ä¸­ï¼ˆè½»é‡èåˆï¼Œé¿å…è¿‡åº¦å½±å“ï¼‰
        df["HI_final"] = np.clip(df["HI"] - df["AI_anomaly"].astype(int) * 10, 0, 100)
    else:
        df["HI_final"] = df["HI"]

    df["Risk_final"] = df["HI_final"].apply(risk)

    return df

df_raw = load_data()

# ================== ä¾§è¾¹æ ï¼šå½•å…¥ ==================
st.sidebar.divider()
st.sidebar.header("ğŸ“ æ¯æ—¥æ•°æ®å½•å…¥")

valve_type = st.sidebar.selectbox("é€‰æ‹©å®‰å…¨é˜€ç±»å‹", ["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"])
date = st.sidebar.date_input("æ—¥æœŸ")
p_now = st.sidebar.number_input("å½“å‰å‹åŠ› p_now (MPa)", 0.0, 2.0, 1.20, 0.01)
p_max = st.sidebar.number_input("å½“æ—¥æœ€é«˜å‹åŠ› p_max (MPa)", 0.0, 2.0, 1.20, 0.01, help="å»ºè®®ï¼šp_max â‰¥ p_nowï¼›è‹¥è¾“å…¥å°äº p_nowï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰ p_now ä¿®æ­£ã€‚")
level = st.sidebar.number_input("æ¶²ä½ level (%)", 0, 100, 60)
temp = st.sidebar.number_input("ç¯å¢ƒæ¸©åº¦ temp (â„ƒ)", -30, 60, 25)
psv_act = st.sidebar.selectbox("æ˜¯å¦åŠ¨ä½œ", ["å¦", "æ˜¯"])
psv_weeping = st.sidebar.selectbox("æ˜¯å¦å¾®æ”¾æ•£/å˜¶å˜¶å£°", ["å¦", "æ˜¯"])

if st.sidebar.button("ä¿å­˜å¹¶è®¡ç®—", use_container_width=True):
    # æ•°æ®æ ¡éªŒï¼šå½“æ—¥æœ€é«˜å‹åŠ›åº” >= å½“å‰å‹åŠ›
    p_now_f = float(p_now)
    p_max_f = float(p_max)
    if p_max_f < p_now_f:
        st.sidebar.warning(f"å·²è‡ªåŠ¨ä¿®æ­£ï¼šp_max({p_max_f:.2f}) < p_now({p_now_f:.2f})ï¼Œå°† p_max è®¾ä¸º {p_now_f:.2f}")
        p_max_f = p_now_f

    # ä½ åŸæ¥çš„å½•å…¥å­—æ®µä¸é€»è¾‘ä¿æŒä¸å˜ï¼Œåªæ›¿æ¢â€œä¿å­˜ä½ç½®â€
    if USE_SUPABASE and supabase is not None:
        supabase.table("psv_data").upsert(
            {
                "date": str(date),
                "valve_type": valve_type,
                "p_now": p_now_f,
                "p_max": p_max_f,
                "level": int(level),
                "temp": int(temp),
                "psv_act": 1 if psv_act == "æ˜¯" else 0,
                "psv_weeping": 1 if psv_weeping == "æ˜¯" else 0,
            },
            on_conflict="date,valve_type",
        ).execute()
        st.sidebar.success("âœ… æ•°æ®å·²ä¿å­˜åˆ° Supabaseï¼ˆäº‘ç«¯ï¼‰")
        st.rerun()
    else:
        new_row = pd.DataFrame(
            [{
                "date": date,
                "valve_type": valve_type,
                "p_now": p_now_f,
                "p_max": p_max_f,
                "level": level,
                "temp": temp,
                "psv_act": 1 if psv_act == "æ˜¯" else 0,
                "psv_weeping": 1 if psv_weeping == "æ˜¯" else 0,
            }]
        )

        df_to_save = pd.concat([df_raw, new_row], ignore_index=True)
        # åŒä¸€é˜€é—¨åŒä¸€å¤©é‡å¤å½•å…¥ï¼šä¿ç•™æœ€åä¸€æ¡
        df_to_save = df_to_save.sort_values(['valve_type','date']).drop_duplicates(subset=['date','valve_type'], keep='last')
        df_to_save.to_csv(DATA_FILE, index=False, encoding="utf-8-sig")
        st.sidebar.success("âœ… æ•°æ®å·²ä¿å­˜ï¼ˆåˆ·æ–°é¡µé¢å¯çœ‹åˆ°æ›´æ–°ï¼‰")

# é‡æ–°åŠ è½½ + è®¡ç®—
df_raw = load_data()
df = compute_scores(df_raw, enable_ai=enable_ai, contamination=contamination)

# ================== ä¸»é¡µé¢ ==================
if len(df) == 0:
    st.info("å½“å‰è¿˜æ²¡æœ‰æ•°æ®ï¼šè¯·åœ¨å·¦ä¾§å½•å…¥å¹¶ç‚¹å‡»ã€ä¿å­˜å¹¶è®¡ç®—ã€‘ã€‚")
    st.stop()

# æ—¥æœŸèŒƒå›´ç­›é€‰ï¼ˆé¢†å¯¼å±•ç¤ºä¼šæ›´åƒâ€œç³»ç»Ÿâ€ï¼‰
min_d, max_d = df["date"].min(), df["date"].max()
colA, colB, colC = st.columns([1, 1, 2])
with colA:
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=min_d, min_value=min_d, max_value=max_d, key="start_date")
with colB:
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=max_d, min_value=min_d, max_value=max_d, key="end_date")
with colC:
    st.caption("æç¤ºï¼šå¦‚æœæ•°æ®é‡å°ï¼Œå»ºè®®å…ˆå½•å…¥ 10â€“30 å¤©")

df_f = df[(df["date"] >= start_date) & (df["date"] <= end_date)].copy()
if len(df_f) == 0:
    st.warning("æ‰€é€‰æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®ã€‚")
    st.stop()

with st.expander("ğŸ“¥ å¯¼å‡ºæ•°æ®ï¼ˆæ‰€é€‰æ—¥æœŸèŒƒå›´ï¼‰", expanded=False):
    csv_bytes = df_f.sort_values(["valve_type", "date"]).to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        label="ä¸‹è½½ CSV",
        data=csv_bytes,
        file_name="psv_data_filtered.csv",
        mime="text/csv",
        use_container_width=True,
    )

# ============ æ€»è§ˆçœ‹æ¿ ============
st.subheader("ğŸ“Œ æ€»è§ˆçœ‹æ¿ï¼ˆæœ€æ–°çŠ¶æ€ï¼‰")

latest_by_valve = df.sort_values(["valve_type", "date"]).groupby("valve_type").tail(1)
cols = st.columns(3)
for i, valve in enumerate(["æ³µåå®‰å…¨é˜€", "å‚¨ç½ä¸»é˜€", "å‚¨ç½è¾…é˜€"]):
    block = latest_by_valve[latest_by_valve["valve_type"] == valve]
    if len(block) == 0:
        cols[i].metric(f"{valve} å½“å‰å¥åº·æŒ‡æ•°", "â€”")
        cols[i].metric(f"{valve} é£é™©ç­‰çº§", "â€”")
        continue
    row = block.iloc[0]
    cols[i].metric(f"{valve} å½“å‰å¥åº·æŒ‡æ•°", f"{row['HI_final']:.1f}")
    cols[i].metric(f"{valve} é£é™©ç­‰çº§", row["Risk_final"])
    cols[i].caption(f"æœ€é«˜å‹åŠ›å æ•´å®šæ¯”ä¾‹ï¼š{row['ratio'] * 100:.1f}% ï½œ åŠ¨ä½œ:{int(row.get('psv_act',0))} å¾®æ”¾æ•£:{int(row.get('psv_weeping',0))}")

st.divider()

# ============ è¯¦æƒ…ï¼šæŒ‰é˜€é—¨è¶‹åŠ¿ ============
st.subheader("ğŸ“ˆ å•é˜€è¶‹åŠ¿ï¼ˆå‹åŠ› & å¥åº·æŒ‡æ•°ï¼‰")
valve_pick = st.selectbox("é€‰æ‹©æŸ¥çœ‹çš„é˜€é—¨", sorted(df_f["valve_type"].unique()), index=0)

vdf = df_f[df_f["valve_type"] == valve_pick].sort_values("date").copy()
vdf["date_dt"] = pd.to_datetime(vdf["date"])

c1, c2 = st.columns(2)
with c1:
    fig, ax = plt.subplots()
    ax.plot(vdf["date_dt"], vdf["p_max"], marker="o", label="p_max")
    if "p_now" in vdf.columns:
        ax.plot(vdf["date_dt"], vdf["p_now"], marker="o", linestyle="--", label="p_now")
    ax.axhline(SET_P, linestyle="--", label="æ•´å®šå‹åŠ› 1.32MPa")
    ax.set_title(f"{valve_pick}ï¼šå½“æ—¥æœ€é«˜å‹åŠ›è¶‹åŠ¿")
    ax.set_ylabel("MPa")
    ax.set_xlabel("æ—¥æœŸ")
    ax.legend()
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.xticks(rotation=30)
    st.pyplot(fig)

with c2:
    fig, ax = plt.subplots()
    ax.plot(vdf["date_dt"], vdf["HI_final"], marker="o")
    ax.set_title(f"{valve_pick}ï¼šå¥åº·æŒ‡æ•°è¶‹åŠ¿ï¼ˆHIï¼ŒAIèåˆï¼‰")
    ax.set_ylabel("HI (0-100)")
    ax.set_xlabel("æ—¥æœŸ")
    ax.set_ylim(0, 100)
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.xticks(rotation=30)
    st.pyplot(fig)

st.divider()

# ============ é«˜çº§å¯è§†åŒ– ============
st.subheader("ğŸ§  é«˜çº§å¯è§†åŒ–")
st.caption("å»ºè®®é˜…è¯»é¡ºåºï¼šâ‘ çƒ­åŠ›å›¾æ‰¾â€œå“ªå¤©å“ªé˜€å˜å·®â€ â†’ â‘¡å¯¹æ¯”å›¾å†³å®šâ€œä¼˜å…ˆå¤„ç†å“ªåªé˜€â€ â†’ â‘¢ç›¸å…³å›¾è§£é‡Šâ€œå‹åŠ›æ¥è¿‘æ•´å®šæ˜¯å¦æ›´å®¹æ˜“åŠ¨ä½œ/å¾®æ”¾æ•£â€ã€‚")

g1, g2, g3 = st.columns(3, gap="small")

# ---- 1) çƒ­åŠ›å›¾ï¼šå¥åº·éšæ—¶é—´ï¼ˆå°å›¾ï¼‰----
with g1:
    st.markdown("**â‘  çƒ­åŠ›å›¾ï¼šå¥åº·éšæ—¶é—´**")
    st.caption("é¢œè‰²è¶Šæ·±ï¼ˆåç´«ï¼‰ä»£è¡¨ HI è¶Šä½ã€‚")

    heat = df_f.pivot_table(index="valve_type", columns="date", values="HI_final", aggfunc="mean")
    fig, ax = plt.subplots(figsize=(4.2, 3.2))
    im = ax.imshow(heat.values, aspect="auto")
    ax.set_title("HI çƒ­åŠ›å›¾")

    ax.set_yticks(range(len(heat.index)))
    ax.set_yticklabels(list(heat.index))

    # æ—¥æœŸå¤ªå¤šæ—¶åšæŠ½æ ·ï¼Œé¿å…å°å›¾æŒ¤çˆ†
    cols = list(heat.columns)
    if len(cols) <= 10:
        tick_idx = list(range(len(cols)))
    else:
        tick_idx = sorted(set(np.linspace(0, len(cols) - 1, 8).round().astype(int).tolist()))
    ax.set_xticks(tick_idx)
    ax.set_xticklabels([pd.to_datetime(cols[i]).strftime("%m-%d") for i in tick_idx], rotation=45, ha="right")

    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label="HI")
    fig.tight_layout()
    st.pyplot(fig, use_container_width=True)

# ---- 2) æ¡å½¢å›¾ï¼šé˜€é—¨å¯¹æ¯”ï¼ˆå°å›¾ï¼‰----
with g2:
    st.markdown("**â‘¡ å¯¹æ¯”ï¼šå¹³å‡HI & é¢„è­¦å¤©æ•°**")
    st.caption("å¹³å‡HIè¶Šä½ã€çº¢/é»„å¤©æ•°è¶Šå¤š â†’ è¶Šä¼˜å…ˆå¤„ç†ã€‚")

    summary = (
        df_f.groupby("valve_type")
        .agg(
            avg_HI=("HI_final", "mean"),
            min_HI=("HI_final", "min"),
            red_days=("Risk_final", lambda s: (s == "ğŸ”´ é«˜é£é™©").sum()),
            yellow_days=("Risk_final", lambda s: (s == "ğŸŸ¡ é¢„è­¦").sum()),
            act_cnt=("psv_act", "sum"),
            weep_cnt=("psv_weeping", "sum"),
        )
        .reset_index()
        .sort_values("avg_HI", ascending=False)
    )

    fig, ax = plt.subplots(figsize=(4.2, 3.2))
    ax.bar(summary["valve_type"], summary["avg_HI"])
    ax.set_title("å¹³å‡HIï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰")
    ax.set_ylabel("avg HI")
    ax.set_ylim(0, 100)
    plt.xticks(rotation=20, ha="right")
    fig.tight_layout()
    st.pyplot(fig, use_container_width=True)

    # å°å›¾ä¸‹æ–¹ç»™ä¸€è¡Œâ€œç»“è®ºæç¤ºâ€ï¼Œé¢†å¯¼æ›´å®¹æ˜“çœ‹æ‡‚
    worst = summary.sort_values("avg_HI").head(1).iloc[0]
    st.info(f"ä¼˜å…ˆå…³æ³¨ï¼š{worst['valve_type']}ï¼ˆå¹³å‡HIâ‰ˆ{worst['avg_HI']:.1f}ï¼Œé«˜é£é™©å¤©æ•°={int(worst['red_days'])}ï¼Œé¢„è­¦å¤©æ•°={int(worst['yellow_days'])}ï¼‰")

# ---- 3) æ•£ç‚¹å›¾ï¼šå‹åŠ› vs æ´»åŠ¨ï¼ˆå°å›¾ï¼‰----
with g3:
    st.markdown("**â‘¢ ç›¸å…³ï¼šå‹åŠ› vs åŠ¨ä½œ/å¾®æ”¾æ•£**")
    st.caption("ç‚¹è¶Šé ä¸Šä»£è¡¨åŠ¨ä½œ/å¾®æ”¾æ•£è¶Šå¤šï¼›ç”¨äºéªŒè¯é˜ˆå€¼è®¾ç½®æ˜¯å¦åˆç†ã€‚")

    sdf = df_f.copy()
    jitter = (np.random.default_rng(0).random(len(sdf)) - 0.5) * 0.06
    y = sdf["Activity"].values + jitter

    fig, ax = plt.subplots(figsize=(4.2, 3.2))
    ax.scatter(sdf["p_max"], y)
    ax.set_title("p_max vs æ´»åŠ¨")
    ax.set_xlabel("p_max (MPa)")
    ax.set_ylabel("æ´»åŠ¨(0/1/2)")
    ax.set_yticks([0, 1, 2])
    ax.set_ylim(-0.3, 2.3)
    fig.tight_layout()
    st.pyplot(fig, use_container_width=True)

    if sdf["p_max"].nunique() > 1 and sdf["Activity"].nunique() > 1:
        corr = np.corrcoef(sdf["p_max"], sdf["Activity"])[0, 1]
        st.metric("ç›¸å…³ç³»æ•°", f"{corr:.2f}")
        if "AI_anomaly" in sdf.columns:
            st.metric("AI å¼‚å¸¸å¤©æ•°", int(sdf["AI_anomaly"].sum()))
    else:
        st.info("æ•°æ®å˜åŒ–ä¸è¶³ï¼Œæš‚æ— æ³•è®¡ç®—ç›¸å…³æ€§ã€‚å»ºè®®å¤šå½•å…¥ä¸€äº›å¤©æ•°ã€‚")

st.divider()

# ============ å†å²è®°å½• ============
st.subheader("ğŸ—‚ å†å²è®°å½•ï¼ˆæœ€è¿‘ 20 æ¡ï¼‰")
show_cols = ["date","valve_type","p_now","p_max","level","temp","psv_act","psv_weeping","HI_final","Risk_final","AI_anomaly","AI_score"]
if not set(show_cols).issubset(df.columns):
    st.dataframe(df.sort_values("date", ascending=False).head(20))
else:
    st.dataframe(df.sort_values("date", ascending=False)[show_cols].head(20))
